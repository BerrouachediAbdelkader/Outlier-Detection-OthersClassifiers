{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import sqrt \n",
    "from pprint import pprint\n",
    "from numpy import array\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>att1</th>\n",
       "      <th>att2</th>\n",
       "      <th>att3</th>\n",
       "      <th>att4</th>\n",
       "      <th>att5</th>\n",
       "      <th>att6</th>\n",
       "      <th>att7</th>\n",
       "      <th>att8</th>\n",
       "      <th>att9</th>\n",
       "      <th>att10</th>\n",
       "      <th>...</th>\n",
       "      <th>att49</th>\n",
       "      <th>att50</th>\n",
       "      <th>att51</th>\n",
       "      <th>att52</th>\n",
       "      <th>att53</th>\n",
       "      <th>att54</th>\n",
       "      <th>att55</th>\n",
       "      <th>att56</th>\n",
       "      <th>att57</th>\n",
       "      <th>outlier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044818</td>\n",
       "      <td>0.125490</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.023955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.017487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046256</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.028886</td>\n",
       "      <td>0.006301</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013536</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011454</td>\n",
       "      <td>0.029985</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>0.010012</td>\n",
       "      <td>0.064836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013216</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.139216</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.032313</td>\n",
       "      <td>0.026135</td>\n",
       "      <td>0.010801</td>\n",
       "      <td>0.121673</td>\n",
       "      <td>0.013751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>0.014664</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.008498</td>\n",
       "      <td>0.030651</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.008008</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>0.142551</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014048</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042641</td>\n",
       "      <td>0.056706</td>\n",
       "      <td>0.058935</td>\n",
       "      <td>0.034653</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013843</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004157</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002303</td>\n",
       "      <td>0.003905</td>\n",
       "      <td>0.011995</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       att1      att2      att3  att4   att5      att6      att7      att8  \\\n",
       "0  0.000000  0.044818  0.125490   0.0  0.032  0.000000  0.000000  0.000000   \n",
       "1  0.046256  0.019608  0.098039   0.0  0.014  0.047619  0.028886  0.006301   \n",
       "2  0.013216  0.000000  0.139216   0.0  0.123  0.032313  0.026135  0.010801   \n",
       "3  0.000000  0.000000  0.000000   0.0  0.063  0.000000  0.042641  0.056706   \n",
       "4  0.000000  0.000000  0.000000   0.0  0.063  0.000000  0.042641  0.056706   \n",
       "\n",
       "       att9     att10   ...        att49     att50  att51     att52     att53  \\\n",
       "0  0.000000  0.000000   ...     0.000000  0.000000    0.0  0.023955  0.000000   \n",
       "1  0.000000  0.051705   ...     0.000000  0.013536    0.0  0.011454  0.029985   \n",
       "2  0.121673  0.013751   ...     0.002281  0.014664    0.0  0.008498  0.030651   \n",
       "3  0.058935  0.034653   ...     0.000000  0.014048    0.0  0.004218  0.000000   \n",
       "4  0.058935  0.034653   ...     0.000000  0.013843    0.0  0.004157  0.000000   \n",
       "\n",
       "      att54     att55     att56     att57  outlier  \n",
       "0  0.000000  0.002502  0.006007  0.017487        1  \n",
       "1  0.002421  0.003735  0.010012  0.064836        1  \n",
       "2  0.000504  0.008008  0.048458  0.142551        1  \n",
       "3  0.000000  0.002303  0.003905  0.011995        1  \n",
       "4  0.000000  0.002303  0.003905  0.011995        1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df=pd.read_csv('SpamBase_withoutdupl_norm_40.csv')  \n",
    "\n",
    "del df['id']\n",
    "del df['Unnamed: 0']\n",
    "df['outlier'] = df.outlier.apply(lambda label: 1 if label == \"'yes'\" else 0)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df to values\n",
    "df = df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GC Forest\n",
    "import argparse\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "sys.path.insert(0, \"lib\")\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.gcforest import GCForest\n",
    "from gcforest.utils.config_utils import load_json\n",
    "config = load_json(\"./examples/SpamBase.json\")   \n",
    "gc = GCForest(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test \n",
    "from sklearn.cross_validation import train_test_split\n",
    "y = df[:,57]\n",
    "X = df[:,0:57]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of class\n",
    "len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:31:49,851][cascade_classifier.fit_transform] X_groups_train.shape=[(2944, 57)],y_train.shape=(2944,),X_groups_test.shape=[(1263, 57)],y_test.shape=(1263,)\n",
      "[ 2018-04-26 23:31:49,853][cascade_classifier.fit_transform] group_dims=[57]\n",
      "[ 2018-04-26 23:31:49,854][cascade_classifier.fit_transform] group_starts=[0]\n",
      "[ 2018-04-26 23:31:49,856][cascade_classifier.fit_transform] group_ends=[57]\n",
      "[ 2018-04-26 23:31:49,857][cascade_classifier.fit_transform] X_train.shape=(2944, 57),X_test.shape=(1263, 57)\n",
      "[ 2018-04-26 23:31:49,860][cascade_classifier.fit_transform] [layer=0] look_indexs=[0], X_cur_train.shape=(2944, 57), X_cur_test.shape=(1263, 57)\n",
      "[ 2018-04-26 23:31:50,760][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_0.predict)=93.22%\n",
      "[ 2018-04-26 23:31:51,935][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_1.predict)=94.24%\n",
      "[ 2018-04-26 23:31:52,980][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:31:54,065][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_3.predict)=94.58%\n",
      "[ 2018-04-26 23:31:55,124][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_4.predict)=93.20%\n",
      "[ 2018-04-26 23:31:56,318][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:31:57,375][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_6.predict)=96.60%\n",
      "[ 2018-04-26 23:31:58,412][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_7.predict)=92.52%\n",
      "[ 2018-04-26 23:31:59,520][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_8.predict)=95.24%\n",
      "[ 2018-04-26 23:32:00,662][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_9.predict)=94.90%\n",
      "[ 2018-04-26 23:32:00,919][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.train_cv.predict)=94.70%\n",
      "[ 2018-04-26 23:32:00,921][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_0 - 10_folds.test.predict)=94.22%\n",
      "[ 2018-04-26 23:32:01,591][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_0.predict)=94.24%\n",
      "[ 2018-04-26 23:32:02,536][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_1.predict)=94.24%\n",
      "[ 2018-04-26 23:32:03,621][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:32:04,692][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:32:05,570][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_4.predict)=94.56%\n",
      "[ 2018-04-26 23:32:06,659][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_5.predict)=92.86%\n",
      "[ 2018-04-26 23:32:07,713][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_6.predict)=96.26%\n",
      "[ 2018-04-26 23:32:08,661][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_7.predict)=94.22%\n",
      "[ 2018-04-26 23:32:09,647][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_8.predict)=97.62%\n",
      "[ 2018-04-26 23:32:10,745][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_9.predict)=95.24%\n",
      "[ 2018-04-26 23:32:11,015][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.train_cv.predict)=95.14%\n",
      "[ 2018-04-26 23:32:11,017][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_1 - 10_folds.test.predict)=94.93%\n",
      "[ 2018-04-26 23:32:11,085][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_0.predict)=89.15%\n",
      "[ 2018-04-26 23:32:11,130][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_1.predict)=87.46%\n",
      "[ 2018-04-26 23:32:11,148][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_2.predict)=88.14%\n",
      "[ 2018-04-26 23:32:11,166][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_3.predict)=86.44%\n",
      "[ 2018-04-26 23:32:11,188][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_4.predict)=90.14%\n",
      "[ 2018-04-26 23:32:11,206][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_5.predict)=86.73%\n",
      "[ 2018-04-26 23:32:11,227][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_6.predict)=92.18%\n",
      "[ 2018-04-26 23:32:11,246][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_7.predict)=88.44%\n",
      "[ 2018-04-26 23:32:11,263][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_8.predict)=87.07%\n",
      "[ 2018-04-26 23:32:11,279][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_9.predict)=88.78%\n",
      "[ 2018-04-26 23:32:11,280][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.train_cv.predict)=88.45%\n",
      "[ 2018-04-26 23:32:11,282][kfold_wrapper.log_eval_metrics] Accuracy(layer_0 - estimator_2 - 10_folds.test.predict)=88.36%\n",
      "[ 2018-04-26 23:32:11,283][cascade_classifier.calc_accuracy] Accuracy(layer_0 - train.classifier_average)=94.74%\n",
      "[ 2018-04-26 23:32:11,284][cascade_classifier.calc_accuracy] Accuracy(layer_0 - test.classifier_average)=94.77%\n",
      "[ 2018-04-26 23:32:11,311][cascade_classifier.fit_transform] [layer=1] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:32:12,193][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_0.predict)=96.95%\n",
      "[ 2018-04-26 23:32:13,274][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_1.predict)=97.97%\n",
      "[ 2018-04-26 23:32:14,331][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_2.predict)=94.92%\n",
      "[ 2018-04-26 23:32:15,440][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_3.predict)=95.59%\n",
      "[ 2018-04-26 23:32:16,557][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:32:17,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_5.predict)=93.20%\n",
      "[ 2018-04-26 23:32:18,840][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_6.predict)=94.56%\n",
      "[ 2018-04-26 23:32:19,952][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_7.predict)=93.88%\n",
      "[ 2018-04-26 23:32:21,043][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:32:22,261][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_9.predict)=93.88%\n",
      "[ 2018-04-26 23:32:22,529][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.train_cv.predict)=95.35%\n",
      "[ 2018-04-26 23:32:22,531][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_0 - 10_folds.test.predict)=94.93%\n",
      "[ 2018-04-26 23:32:23,391][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_0.predict)=96.27%\n",
      "[ 2018-04-26 23:32:24,494][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_1.predict)=95.93%\n",
      "[ 2018-04-26 23:32:25,610][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_2.predict)=93.90%\n",
      "[ 2018-04-26 23:32:26,711][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:32:27,885][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_4.predict)=94.56%\n",
      "[ 2018-04-26 23:32:28,955][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:32:30,070][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:32:31,142][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_7.predict)=94.22%\n",
      "[ 2018-04-26 23:32:32,316][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_8.predict)=94.56%\n",
      "[ 2018-04-26 23:32:33,422][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_9.predict)=96.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:32:33,681][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.train_cv.predict)=95.35%\n",
      "[ 2018-04-26 23:32:33,683][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_1 - 10_folds.test.predict)=94.54%\n",
      "[ 2018-04-26 23:32:33,714][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:32:33,741][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_1.predict)=93.56%\n",
      "[ 2018-04-26 23:32:33,759][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:32:33,776][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:32:33,792][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_4.predict)=93.88%\n",
      "[ 2018-04-26 23:32:33,808][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:32:33,823][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:32:33,837][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_7.predict)=96.26%\n",
      "[ 2018-04-26 23:32:33,850][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:32:33,863][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:32:33,865][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.train_cv.predict)=95.07%\n",
      "[ 2018-04-26 23:32:33,866][kfold_wrapper.log_eval_metrics] Accuracy(layer_1 - estimator_2 - 10_folds.test.predict)=95.25%\n",
      "[ 2018-04-26 23:32:33,867][cascade_classifier.calc_accuracy] Accuracy(layer_1 - train.classifier_average)=95.24%\n",
      "[ 2018-04-26 23:32:33,869][cascade_classifier.calc_accuracy] Accuracy(layer_1 - test.classifier_average)=95.17%\n",
      "[ 2018-04-26 23:32:33,871][cascade_classifier.fit_transform] [layer=2] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:32:34,541][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_0.predict)=94.92%\n",
      "[ 2018-04-26 23:32:35,631][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_1.predict)=93.22%\n",
      "[ 2018-04-26 23:32:36,842][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_2.predict)=95.25%\n",
      "[ 2018-04-26 23:32:37,929][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:32:39,023][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_4.predict)=95.92%\n",
      "[ 2018-04-26 23:32:40,091][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_5.predict)=96.60%\n",
      "[ 2018-04-26 23:32:41,150][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:32:42,365][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_7.predict)=94.90%\n",
      "[ 2018-04-26 23:32:43,446][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_8.predict)=93.88%\n",
      "[ 2018-04-26 23:32:44,588][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:32:44,844][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.train_cv.predict)=95.21%\n",
      "[ 2018-04-26 23:32:44,846][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_0 - 10_folds.test.predict)=94.46%\n",
      "[ 2018-04-26 23:32:45,717][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_0.predict)=95.59%\n",
      "[ 2018-04-26 23:32:46,816][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_1.predict)=93.90%\n",
      "[ 2018-04-26 23:32:47,932][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_2.predict)=95.93%\n",
      "[ 2018-04-26 23:32:49,082][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:32:50,172][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_4.predict)=95.92%\n",
      "[ 2018-04-26 23:32:51,233][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_5.predict)=96.94%\n",
      "[ 2018-04-26 23:32:52,334][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_6.predict)=95.58%\n",
      "[ 2018-04-26 23:32:53,446][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_7.predict)=93.20%\n",
      "[ 2018-04-26 23:32:54,391][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_8.predict)=94.56%\n",
      "[ 2018-04-26 23:32:55,498][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:32:55,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.train_cv.predict)=95.28%\n",
      "[ 2018-04-26 23:32:55,737][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:32:55,761][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_0.predict)=95.25%\n",
      "[ 2018-04-26 23:32:55,779][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_1.predict)=94.58%\n",
      "[ 2018-04-26 23:32:55,795][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:32:55,810][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_3.predict)=94.58%\n",
      "[ 2018-04-26 23:32:55,825][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_4.predict)=94.56%\n",
      "[ 2018-04-26 23:32:55,838][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_5.predict)=96.60%\n",
      "[ 2018-04-26 23:32:55,852][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:32:55,864][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:32:55,876][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_8.predict)=96.60%\n",
      "[ 2018-04-26 23:32:55,888][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_9.predict)=93.54%\n",
      "[ 2018-04-26 23:32:55,890][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.train_cv.predict)=95.31%\n",
      "[ 2018-04-26 23:32:55,891][kfold_wrapper.log_eval_metrics] Accuracy(layer_2 - estimator_2 - 10_folds.test.predict)=94.77%\n",
      "[ 2018-04-26 23:32:55,892][cascade_classifier.calc_accuracy] Accuracy(layer_2 - train.classifier_average)=95.38%\n",
      "[ 2018-04-26 23:32:55,893][cascade_classifier.calc_accuracy] Accuracy(layer_2 - test.classifier_average)=94.70%\n",
      "[ 2018-04-26 23:32:55,895][cascade_classifier.fit_transform] [layer=3] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:32:56,712][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_0.predict)=93.56%\n",
      "[ 2018-04-26 23:32:57,832][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_1.predict)=94.92%\n",
      "[ 2018-04-26 23:32:58,941][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_2.predict)=95.93%\n",
      "[ 2018-04-26 23:33:00,042][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:33:00,985][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_4.predict)=94.22%\n",
      "[ 2018-04-26 23:33:02,210][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_5.predict)=96.60%\n",
      "[ 2018-04-26 23:33:03,256][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_6.predict)=94.90%\n",
      "[ 2018-04-26 23:33:04,350][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_7.predict)=94.90%\n",
      "[ 2018-04-26 23:33:05,394][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:33:06,468][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_9.predict)=96.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:33:06,709][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.train_cv.predict)=95.31%\n",
      "[ 2018-04-26 23:33:06,711][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_0 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:33:07,393][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:33:08,525][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_1.predict)=95.59%\n",
      "[ 2018-04-26 23:33:09,589][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:33:10,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:33:11,545][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_4.predict)=95.24%\n",
      "[ 2018-04-26 23:33:12,743][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_5.predict)=95.58%\n",
      "[ 2018-04-26 23:33:13,796][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:33:14,856][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:33:15,978][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:33:17,059][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_9.predict)=95.24%\n",
      "[ 2018-04-26 23:33:17,295][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.train_cv.predict)=95.45%\n",
      "[ 2018-04-26 23:33:17,297][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_1 - 10_folds.test.predict)=94.54%\n",
      "[ 2018-04-26 23:33:17,327][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_0.predict)=95.25%\n",
      "[ 2018-04-26 23:33:17,348][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_1.predict)=95.59%\n",
      "[ 2018-04-26 23:33:17,367][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_2.predict)=93.22%\n",
      "[ 2018-04-26 23:33:17,385][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_3.predict)=93.22%\n",
      "[ 2018-04-26 23:33:17,408][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_4.predict)=94.22%\n",
      "[ 2018-04-26 23:33:17,428][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_5.predict)=96.60%\n",
      "[ 2018-04-26 23:33:17,447][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:33:17,464][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:33:17,481][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:33:17,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_9.predict)=94.90%\n",
      "[ 2018-04-26 23:33:17,498][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.train_cv.predict)=95.18%\n",
      "[ 2018-04-26 23:33:17,498][kfold_wrapper.log_eval_metrics] Accuracy(layer_3 - estimator_2 - 10_folds.test.predict)=94.46%\n",
      "[ 2018-04-26 23:33:17,500][cascade_classifier.calc_accuracy] Accuracy(layer_3 - train.classifier_average)=95.41%\n",
      "[ 2018-04-26 23:33:17,501][cascade_classifier.calc_accuracy] Accuracy(layer_3 - test.classifier_average)=94.54%\n",
      "[ 2018-04-26 23:33:17,503][cascade_classifier.fit_transform] [layer=4] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:33:18,309][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_0.predict)=96.27%\n",
      "[ 2018-04-26 23:33:19,440][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_1.predict)=94.58%\n",
      "[ 2018-04-26 23:33:20,511][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:33:21,701][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:33:22,793][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_4.predict)=93.88%\n",
      "[ 2018-04-26 23:33:23,888][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_5.predict)=94.90%\n",
      "[ 2018-04-26 23:33:24,936][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:33:26,018][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_7.predict)=96.26%\n",
      "[ 2018-04-26 23:33:27,219][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_8.predict)=95.24%\n",
      "[ 2018-04-26 23:33:28,355][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_9.predict)=94.22%\n",
      "[ 2018-04-26 23:33:28,610][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.train_cv.predict)=95.28%\n",
      "[ 2018-04-26 23:33:28,612][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:33:29,461][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_0.predict)=97.29%\n",
      "[ 2018-04-26 23:33:30,520][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_1.predict)=96.61%\n",
      "[ 2018-04-26 23:33:31,564][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_2.predict)=95.93%\n",
      "[ 2018-04-26 23:33:32,655][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:33:33,795][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_4.predict)=95.92%\n",
      "[ 2018-04-26 23:33:34,891][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_5.predict)=93.88%\n",
      "[ 2018-04-26 23:33:35,974][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_6.predict)=94.90%\n",
      "[ 2018-04-26 23:33:37,091][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:33:38,219][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_8.predict)=93.54%\n",
      "[ 2018-04-26 23:33:39,307][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:33:39,547][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.train_cv.predict)=95.48%\n",
      "[ 2018-04-26 23:33:39,548][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_1 - 10_folds.test.predict)=94.54%\n",
      "[ 2018-04-26 23:33:39,580][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:33:39,602][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_1.predict)=97.29%\n",
      "[ 2018-04-26 23:33:39,622][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_2.predict)=94.92%\n",
      "[ 2018-04-26 23:33:39,640][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:33:39,660][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_4.predict)=96.94%\n",
      "[ 2018-04-26 23:33:39,677][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:33:39,693][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:33:39,708][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_7.predict)=94.22%\n",
      "[ 2018-04-26 23:33:39,721][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_8.predict)=93.88%\n",
      "[ 2018-04-26 23:33:39,735][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_9.predict)=94.90%\n",
      "[ 2018-04-26 23:33:39,736][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.train_cv.predict)=95.45%\n",
      "[ 2018-04-26 23:33:39,738][kfold_wrapper.log_eval_metrics] Accuracy(layer_4 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:33:39,740][cascade_classifier.calc_accuracy] Accuracy(layer_4 - train.classifier_average)=95.41%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:33:39,741][cascade_classifier.calc_accuracy] Accuracy(layer_4 - test.classifier_average)=94.54%\n",
      "[ 2018-04-26 23:33:39,744][cascade_classifier.fit_transform] [layer=5] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:33:40,510][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_0.predict)=94.58%\n",
      "[ 2018-04-26 23:33:41,627][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_1.predict)=96.95%\n",
      "[ 2018-04-26 23:33:42,735][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_2.predict)=94.92%\n",
      "[ 2018-04-26 23:33:43,839][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:33:44,947][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:33:46,046][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:33:47,110][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_6.predict)=93.54%\n",
      "[ 2018-04-26 23:33:48,219][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:33:49,279][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_8.predict)=95.58%\n",
      "[ 2018-04-26 23:33:50,374][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_9.predict)=96.60%\n",
      "[ 2018-04-26 23:33:50,637][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:33:50,639][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:33:51,455][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_0.predict)=96.27%\n",
      "[ 2018-04-26 23:33:52,644][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:33:53,694][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_2.predict)=95.25%\n",
      "[ 2018-04-26 23:33:54,791][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_3.predict)=94.24%\n",
      "[ 2018-04-26 23:33:55,855][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_4.predict)=96.60%\n",
      "[ 2018-04-26 23:33:56,958][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:33:58,089][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:33:59,190][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:34:00,295][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:34:01,313][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:34:01,564][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.train_cv.predict)=95.48%\n",
      "[ 2018-04-26 23:34:01,566][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_1 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:34:01,598][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_0.predict)=93.90%\n",
      "[ 2018-04-26 23:34:01,620][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:34:01,641][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:34:01,659][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_3.predict)=96.95%\n",
      "[ 2018-04-26 23:34:01,677][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_4.predict)=94.90%\n",
      "[ 2018-04-26 23:34:01,693][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:34:01,709][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:34:01,723][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_7.predict)=94.90%\n",
      "[ 2018-04-26 23:34:01,737][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_8.predict)=95.92%\n",
      "[ 2018-04-26 23:34:01,751][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_9.predict)=96.94%\n",
      "[ 2018-04-26 23:34:01,752][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.train_cv.predict)=95.41%\n",
      "[ 2018-04-26 23:34:01,753][kfold_wrapper.log_eval_metrics] Accuracy(layer_5 - estimator_2 - 10_folds.test.predict)=94.54%\n",
      "[ 2018-04-26 23:34:01,754][cascade_classifier.calc_accuracy] Accuracy(layer_5 - train.classifier_average)=95.48%\n",
      "[ 2018-04-26 23:34:01,755][cascade_classifier.calc_accuracy] Accuracy(layer_5 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:34:01,756][cascade_classifier.fit_transform] [layer=6] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:34:02,579][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_0.predict)=95.25%\n",
      "[ 2018-04-26 23:34:03,672][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_1.predict)=97.29%\n",
      "[ 2018-04-26 23:34:04,747][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_2.predict)=94.92%\n",
      "[ 2018-04-26 23:34:05,849][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_3.predict)=97.29%\n",
      "[ 2018-04-26 23:34:06,909][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_4.predict)=94.56%\n",
      "[ 2018-04-26 23:34:08,003][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_5.predict)=95.92%\n",
      "[ 2018-04-26 23:34:09,126][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_6.predict)=93.54%\n",
      "[ 2018-04-26 23:34:10,232][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_7.predict)=94.56%\n",
      "[ 2018-04-26 23:34:11,320][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:34:12,448][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_9.predict)=96.94%\n",
      "[ 2018-04-26 23:34:12,689][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.train_cv.predict)=95.45%\n",
      "[ 2018-04-26 23:34:12,691][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_0 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:34:13,589][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_0.predict)=95.25%\n",
      "[ 2018-04-26 23:34:14,822][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_1.predict)=95.59%\n",
      "[ 2018-04-26 23:34:15,971][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:34:17,020][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_3.predict)=93.90%\n",
      "[ 2018-04-26 23:34:18,112][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_4.predict)=96.60%\n",
      "[ 2018-04-26 23:34:19,343][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:34:20,430][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_6.predict)=96.94%\n",
      "[ 2018-04-26 23:34:21,498][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_7.predict)=93.54%\n",
      "[ 2018-04-26 23:34:22,597][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_8.predict)=96.26%\n",
      "[ 2018-04-26 23:34:23,759][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_9.predict)=93.88%\n",
      "[ 2018-04-26 23:34:24,028][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.train_cv.predict)=95.38%\n",
      "[ 2018-04-26 23:34:24,030][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:34:24,059][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_0.predict)=96.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:34:24,081][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_1.predict)=95.93%\n",
      "[ 2018-04-26 23:34:24,102][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_2.predict)=96.95%\n",
      "[ 2018-04-26 23:34:24,120][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:34:24,138][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_4.predict)=94.22%\n",
      "[ 2018-04-26 23:34:24,156][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_5.predict)=95.24%\n",
      "[ 2018-04-26 23:34:24,172][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_6.predict)=94.56%\n",
      "[ 2018-04-26 23:34:24,187][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_7.predict)=94.56%\n",
      "[ 2018-04-26 23:34:24,202][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_8.predict)=96.60%\n",
      "[ 2018-04-26 23:34:24,217][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_9.predict)=94.56%\n",
      "[ 2018-04-26 23:34:24,218][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.train_cv.predict)=95.48%\n",
      "[ 2018-04-26 23:34:24,220][kfold_wrapper.log_eval_metrics] Accuracy(layer_6 - estimator_2 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:34:24,221][cascade_classifier.calc_accuracy] Accuracy(layer_6 - train.classifier_average)=95.48%\n",
      "[ 2018-04-26 23:34:24,222][cascade_classifier.calc_accuracy] Accuracy(layer_6 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:34:24,224][cascade_classifier.fit_transform] [layer=7] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:34:25,076][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_0.predict)=95.59%\n",
      "[ 2018-04-26 23:34:26,165][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:34:27,261][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_2.predict)=93.56%\n",
      "[ 2018-04-26 23:34:28,362][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:34:29,455][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_4.predict)=96.26%\n",
      "[ 2018-04-26 23:34:30,613][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_5.predict)=96.60%\n",
      "[ 2018-04-26 23:34:31,695][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_6.predict)=96.60%\n",
      "[ 2018-04-26 23:34:32,772][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_7.predict)=94.22%\n",
      "[ 2018-04-26 23:34:33,869][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_8.predict)=94.56%\n",
      "[ 2018-04-26 23:34:34,974][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_9.predict)=95.92%\n",
      "[ 2018-04-26 23:34:35,309][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.train_cv.predict)=95.48%\n",
      "[ 2018-04-26 23:34:35,311][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:34:36,140][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:34:37,198][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_1.predict)=94.58%\n",
      "[ 2018-04-26 23:34:38,360][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_2.predict)=93.90%\n",
      "[ 2018-04-26 23:34:39,439][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:34:40,522][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:34:41,696][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_5.predict)=97.28%\n",
      "[ 2018-04-26 23:34:42,832][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_6.predict)=97.62%\n",
      "[ 2018-04-26 23:34:43,921][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:34:44,997][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_8.predict)=94.56%\n",
      "[ 2018-04-26 23:34:46,062][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_9.predict)=93.88%\n",
      "[ 2018-04-26 23:34:46,326][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.train_cv.predict)=95.52%\n",
      "[ 2018-04-26 23:34:46,328][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:34:46,359][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_0.predict)=97.63%\n",
      "[ 2018-04-26 23:34:46,380][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_1.predict)=95.59%\n",
      "[ 2018-04-26 23:34:46,400][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_2.predict)=93.90%\n",
      "[ 2018-04-26 23:34:46,419][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:34:46,436][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_4.predict)=94.22%\n",
      "[ 2018-04-26 23:34:46,452][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_5.predict)=93.54%\n",
      "[ 2018-04-26 23:34:46,468][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_6.predict)=96.26%\n",
      "[ 2018-04-26 23:34:46,482][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:34:46,496][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_8.predict)=96.26%\n",
      "[ 2018-04-26 23:34:46,510][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:34:46,512][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.train_cv.predict)=95.55%\n",
      "[ 2018-04-26 23:34:46,513][kfold_wrapper.log_eval_metrics] Accuracy(layer_7 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:34:46,514][cascade_classifier.calc_accuracy] Accuracy(layer_7 - train.classifier_average)=95.52%\n",
      "[ 2018-04-26 23:34:46,515][cascade_classifier.calc_accuracy] Accuracy(layer_7 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:34:46,517][cascade_classifier.fit_transform] [layer=8] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:34:47,324][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_0.predict)=94.58%\n",
      "[ 2018-04-26 23:34:48,410][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:34:49,487][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_2.predict)=96.61%\n",
      "[ 2018-04-26 23:34:50,540][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:34:51,565][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_4.predict)=94.56%\n",
      "[ 2018-04-26 23:34:52,721][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_5.predict)=95.24%\n",
      "[ 2018-04-26 23:34:53,772][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:34:54,873][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_7.predict)=94.56%\n",
      "[ 2018-04-26 23:34:56,011][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_8.predict)=95.58%\n",
      "[ 2018-04-26 23:34:57,128][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_9.predict)=95.92%\n",
      "[ 2018-04-26 23:34:57,397][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.train_cv.predict)=95.45%\n",
      "[ 2018-04-26 23:34:57,398][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:34:58,220][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_0.predict)=94.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:34:59,332][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_1.predict)=94.58%\n",
      "[ 2018-04-26 23:35:00,425][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:35:01,372][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_3.predict)=93.90%\n",
      "[ 2018-04-26 23:35:02,449][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:35:03,471][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_5.predict)=97.96%\n",
      "[ 2018-04-26 23:35:04,498][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_6.predict)=94.90%\n",
      "[ 2018-04-26 23:35:05,466][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:35:06,570][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_8.predict)=95.58%\n",
      "[ 2018-04-26 23:35:07,643][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_9.predict)=95.24%\n",
      "[ 2018-04-26 23:35:07,909][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:35:07,911][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:07,943][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:35:07,967][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_1.predict)=93.56%\n",
      "[ 2018-04-26 23:35:07,989][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_2.predict)=95.93%\n",
      "[ 2018-04-26 23:35:08,009][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_3.predict)=97.63%\n",
      "[ 2018-04-26 23:35:08,029][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_4.predict)=92.18%\n",
      "[ 2018-04-26 23:35:08,045][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_5.predict)=95.24%\n",
      "[ 2018-04-26 23:35:08,063][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_6.predict)=96.94%\n",
      "[ 2018-04-26 23:35:08,079][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:35:08,095][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_8.predict)=95.58%\n",
      "[ 2018-04-26 23:35:08,109][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:35:08,111][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.train_cv.predict)=95.55%\n",
      "[ 2018-04-26 23:35:08,112][kfold_wrapper.log_eval_metrics] Accuracy(layer_8 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:08,113][cascade_classifier.calc_accuracy] Accuracy(layer_8 - train.classifier_average)=95.52%\n",
      "[ 2018-04-26 23:35:08,114][cascade_classifier.calc_accuracy] Accuracy(layer_8 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:35:08,116][cascade_classifier.fit_transform] [layer=9] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:35:08,892][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_0.predict)=96.95%\n",
      "[ 2018-04-26 23:35:09,948][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_1.predict)=95.93%\n",
      "[ 2018-04-26 23:35:11,053][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_2.predict)=96.61%\n",
      "[ 2018-04-26 23:35:12,152][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_3.predict)=96.61%\n",
      "[ 2018-04-26 23:35:13,252][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_4.predict)=96.26%\n",
      "[ 2018-04-26 23:35:14,316][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_5.predict)=93.88%\n",
      "[ 2018-04-26 23:35:15,403][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_6.predict)=94.90%\n",
      "[ 2018-04-26 23:35:16,494][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_7.predict)=93.88%\n",
      "[ 2018-04-26 23:35:17,569][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:35:18,654][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_9.predict)=93.54%\n",
      "[ 2018-04-26 23:35:18,900][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.train_cv.predict)=95.55%\n",
      "[ 2018-04-26 23:35:18,901][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:19,758][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_0.predict)=96.95%\n",
      "[ 2018-04-26 23:35:20,933][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:35:22,032][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_2.predict)=94.24%\n",
      "[ 2018-04-26 23:35:23,155][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_3.predict)=95.25%\n",
      "[ 2018-04-26 23:35:24,243][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_4.predict)=96.94%\n",
      "[ 2018-04-26 23:35:25,429][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_5.predict)=95.58%\n",
      "[ 2018-04-26 23:35:26,511][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_6.predict)=94.22%\n",
      "[ 2018-04-26 23:35:27,604][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_7.predict)=95.24%\n",
      "[ 2018-04-26 23:35:28,668][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:35:29,709][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:35:29,968][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.train_cv.predict)=95.62%\n",
      "[ 2018-04-26 23:35:29,970][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:30,006][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:35:30,029][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_1.predict)=94.24%\n",
      "[ 2018-04-26 23:35:30,052][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_2.predict)=96.95%\n",
      "[ 2018-04-26 23:35:30,073][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_3.predict)=96.27%\n",
      "[ 2018-04-26 23:35:30,093][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_4.predict)=96.26%\n",
      "[ 2018-04-26 23:35:30,109][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:35:30,124][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_6.predict)=93.54%\n",
      "[ 2018-04-26 23:35:30,138][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_7.predict)=94.90%\n",
      "[ 2018-04-26 23:35:30,153][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_8.predict)=95.92%\n",
      "[ 2018-04-26 23:35:30,169][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_9.predict)=96.60%\n",
      "[ 2018-04-26 23:35:30,170][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.train_cv.predict)=95.52%\n",
      "[ 2018-04-26 23:35:30,171][kfold_wrapper.log_eval_metrics] Accuracy(layer_9 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:30,172][cascade_classifier.calc_accuracy] Accuracy(layer_9 - train.classifier_average)=95.52%\n",
      "[ 2018-04-26 23:35:30,173][cascade_classifier.calc_accuracy] Accuracy(layer_9 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:35:30,175][cascade_classifier.fit_transform] [layer=10] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:35:31,043][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_0.predict)=93.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:35:32,105][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_1.predict)=96.61%\n",
      "[ 2018-04-26 23:35:33,200][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_2.predict)=95.93%\n",
      "[ 2018-04-26 23:35:34,283][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_3.predict)=94.24%\n",
      "[ 2018-04-26 23:35:35,376][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_4.predict)=96.94%\n",
      "[ 2018-04-26 23:35:36,444][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_5.predict)=95.92%\n",
      "[ 2018-04-26 23:35:37,651][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:35:38,788][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:35:39,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:35:41,031][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_9.predict)=96.60%\n",
      "[ 2018-04-26 23:35:41,290][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:35:41,291][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:42,131][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:35:43,225][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:35:44,373][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_2.predict)=93.90%\n",
      "[ 2018-04-26 23:35:45,471][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_3.predict)=93.90%\n",
      "[ 2018-04-26 23:35:46,589][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_4.predict)=96.60%\n",
      "[ 2018-04-26 23:35:47,706][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_5.predict)=96.94%\n",
      "[ 2018-04-26 23:35:48,737][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_6.predict)=96.94%\n",
      "[ 2018-04-26 23:35:49,795][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_7.predict)=93.20%\n",
      "[ 2018-04-26 23:35:50,856][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:35:51,997][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_9.predict)=94.56%\n",
      "[ 2018-04-26 23:35:52,251][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.train_cv.predict)=95.52%\n",
      "[ 2018-04-26 23:35:52,252][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:52,285][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:35:52,308][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:35:52,329][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:35:52,348][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_3.predict)=96.27%\n",
      "[ 2018-04-26 23:35:52,367][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_4.predict)=95.24%\n",
      "[ 2018-04-26 23:35:52,384][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_5.predict)=95.92%\n",
      "[ 2018-04-26 23:35:52,400][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:35:52,416][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_7.predict)=94.56%\n",
      "[ 2018-04-26 23:35:52,433][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_8.predict)=95.24%\n",
      "[ 2018-04-26 23:35:52,449][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_9.predict)=95.92%\n",
      "[ 2018-04-26 23:35:52,450][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:35:52,452][kfold_wrapper.log_eval_metrics] Accuracy(layer_10 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:35:52,453][cascade_classifier.calc_accuracy] Accuracy(layer_10 - train.classifier_average)=95.62%\n",
      "[ 2018-04-26 23:35:52,454][cascade_classifier.calc_accuracy] Accuracy(layer_10 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:35:52,456][cascade_classifier.fit_transform] [layer=11] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:35:53,252][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_0.predict)=95.93%\n",
      "[ 2018-04-26 23:35:54,323][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:35:55,479][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_2.predict)=95.59%\n",
      "[ 2018-04-26 23:35:56,560][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_3.predict)=94.92%\n",
      "[ 2018-04-26 23:35:57,612][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_4.predict)=93.88%\n",
      "[ 2018-04-26 23:35:58,723][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_5.predict)=95.24%\n",
      "[ 2018-04-26 23:35:59,801][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:36:00,914][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_7.predict)=96.60%\n",
      "[ 2018-04-26 23:36:02,019][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_8.predict)=96.94%\n",
      "[ 2018-04-26 23:36:03,133][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:36:03,387][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:36:03,389][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_0 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:36:04,256][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_0.predict)=96.61%\n",
      "[ 2018-04-26 23:36:05,350][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:36:06,448][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_2.predict)=96.27%\n",
      "[ 2018-04-26 23:36:07,563][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_3.predict)=94.24%\n",
      "[ 2018-04-26 23:36:08,627][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_4.predict)=96.60%\n",
      "[ 2018-04-26 23:36:09,737][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_5.predict)=94.90%\n",
      "[ 2018-04-26 23:36:10,806][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_6.predict)=95.58%\n",
      "[ 2018-04-26 23:36:11,952][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_7.predict)=95.24%\n",
      "[ 2018-04-26 23:36:13,027][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_8.predict)=95.92%\n",
      "[ 2018-04-26 23:36:14,103][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_9.predict)=94.22%\n",
      "[ 2018-04-26 23:36:14,360][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:36:14,361][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:36:14,392][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_0.predict)=94.92%\n",
      "[ 2018-04-26 23:36:14,414][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:36:14,435][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_2.predict)=94.92%\n",
      "[ 2018-04-26 23:36:14,453][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_3.predict)=95.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:36:14,472][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_4.predict)=96.60%\n",
      "[ 2018-04-26 23:36:14,495][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_5.predict)=94.90%\n",
      "[ 2018-04-26 23:36:14,517][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_6.predict)=95.58%\n",
      "[ 2018-04-26 23:36:14,536][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_7.predict)=95.58%\n",
      "[ 2018-04-26 23:36:14,555][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_8.predict)=94.90%\n",
      "[ 2018-04-26 23:36:14,572][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:36:14,574][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:36:14,575][kfold_wrapper.log_eval_metrics] Accuracy(layer_11 - estimator_2 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:36:14,577][cascade_classifier.calc_accuracy] Accuracy(layer_11 - train.classifier_average)=95.62%\n",
      "[ 2018-04-26 23:36:14,578][cascade_classifier.calc_accuracy] Accuracy(layer_11 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:36:14,580][cascade_classifier.fit_transform] [layer=12] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:36:15,417][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_0.predict)=93.90%\n",
      "[ 2018-04-26 23:36:16,453][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_1.predict)=93.56%\n",
      "[ 2018-04-26 23:36:17,548][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_2.predict)=97.29%\n",
      "[ 2018-04-26 23:36:18,651][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_3.predict)=95.93%\n",
      "[ 2018-04-26 23:36:19,791][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_4.predict)=96.26%\n",
      "[ 2018-04-26 23:36:20,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:36:22,013][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:36:23,190][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_7.predict)=96.26%\n",
      "[ 2018-04-26 23:36:24,296][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_8.predict)=95.24%\n",
      "[ 2018-04-26 23:36:25,342][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:36:25,605][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.train_cv.predict)=95.62%\n",
      "[ 2018-04-26 23:36:25,606][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_0 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:36:26,396][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_0.predict)=95.59%\n",
      "[ 2018-04-26 23:36:27,490][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_1.predict)=97.29%\n",
      "[ 2018-04-26 23:36:28,595][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_2.predict)=96.95%\n",
      "[ 2018-04-26 23:36:29,828][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_3.predict)=96.95%\n",
      "[ 2018-04-26 23:36:30,901][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_4.predict)=94.22%\n",
      "[ 2018-04-26 23:36:32,016][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_5.predict)=93.88%\n",
      "[ 2018-04-26 23:36:33,146][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:36:34,223][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:36:35,318][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_8.predict)=94.22%\n",
      "[ 2018-04-26 23:36:36,392][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:36:36,645][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.train_cv.predict)=95.65%\n",
      "[ 2018-04-26 23:36:36,647][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_1 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:36:36,680][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_0.predict)=93.22%\n",
      "[ 2018-04-26 23:36:36,711][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_1.predict)=96.27%\n",
      "[ 2018-04-26 23:36:36,733][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_2.predict)=97.63%\n",
      "[ 2018-04-26 23:36:36,754][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_3.predict)=94.58%\n",
      "[ 2018-04-26 23:36:36,773][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_4.predict)=97.62%\n",
      "[ 2018-04-26 23:36:36,792][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_5.predict)=96.26%\n",
      "[ 2018-04-26 23:36:36,807][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_6.predict)=93.20%\n",
      "[ 2018-04-26 23:36:36,824][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_7.predict)=95.24%\n",
      "[ 2018-04-26 23:36:36,841][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_8.predict)=96.26%\n",
      "[ 2018-04-26 23:36:36,857][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_9.predict)=95.58%\n",
      "[ 2018-04-26 23:36:36,859][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.train_cv.predict)=95.58%\n",
      "[ 2018-04-26 23:36:36,860][kfold_wrapper.log_eval_metrics] Accuracy(layer_12 - estimator_2 - 10_folds.test.predict)=94.54%\n",
      "[ 2018-04-26 23:36:36,861][cascade_classifier.calc_accuracy] Accuracy(layer_12 - train.classifier_average)=95.62%\n",
      "[ 2018-04-26 23:36:36,862][cascade_classifier.calc_accuracy] Accuracy(layer_12 - test.classifier_average)=94.62%\n",
      "[ 2018-04-26 23:36:36,865][cascade_classifier.fit_transform] [layer=13] look_indexs=[0], X_cur_train.shape=(2944, 63), X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:36:37,688][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_0.predict)=94.92%\n",
      "[ 2018-04-26 23:36:38,745][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_1.predict)=94.92%\n",
      "[ 2018-04-26 23:36:39,819][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_2.predict)=95.25%\n",
      "[ 2018-04-26 23:36:40,999][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_3.predict)=97.97%\n",
      "[ 2018-04-26 23:36:42,119][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:36:43,218][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_5.predict)=95.92%\n",
      "[ 2018-04-26 23:36:44,293][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_6.predict)=95.92%\n",
      "[ 2018-04-26 23:36:45,388][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_7.predict)=95.92%\n",
      "[ 2018-04-26 23:36:46,468][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_8.predict)=95.58%\n",
      "[ 2018-04-26 23:36:47,692][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_9.predict)=94.22%\n",
      "[ 2018-04-26 23:36:47,940][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.train_cv.predict)=95.62%\n",
      "[ 2018-04-26 23:36:47,942][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_0 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:36:48,766][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_0.predict)=95.59%\n",
      "[ 2018-04-26 23:36:49,761][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_1.predict)=93.90%\n",
      "[ 2018-04-26 23:36:50,862][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_2.predict)=97.63%\n",
      "[ 2018-04-26 23:36:51,958][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_3.predict)=96.61%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:36:53,069][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_4.predict)=95.58%\n",
      "[ 2018-04-26 23:36:54,293][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_5.predict)=94.56%\n",
      "[ 2018-04-26 23:36:55,365][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:36:56,463][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_7.predict)=96.94%\n",
      "[ 2018-04-26 23:36:57,511][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_8.predict)=93.88%\n",
      "[ 2018-04-26 23:36:58,590][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_9.predict)=96.26%\n",
      "[ 2018-04-26 23:36:58,828][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.train_cv.predict)=95.62%\n",
      "[ 2018-04-26 23:36:58,829][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_1 - 10_folds.test.predict)=94.62%\n",
      "[ 2018-04-26 23:36:58,855][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_0.predict)=95.59%\n",
      "[ 2018-04-26 23:36:58,876][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_1.predict)=95.25%\n",
      "[ 2018-04-26 23:36:58,896][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_2.predict)=95.25%\n",
      "[ 2018-04-26 23:36:58,914][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_3.predict)=97.29%\n",
      "[ 2018-04-26 23:36:58,931][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_4.predict)=95.24%\n",
      "[ 2018-04-26 23:36:58,947][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_5.predict)=95.92%\n",
      "[ 2018-04-26 23:36:58,961][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_6.predict)=95.24%\n",
      "[ 2018-04-26 23:36:58,978][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_7.predict)=96.60%\n",
      "[ 2018-04-26 23:36:58,994][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_8.predict)=92.86%\n",
      "[ 2018-04-26 23:36:59,010][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_9.predict)=96.94%\n",
      "[ 2018-04-26 23:36:59,012][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.train_cv.predict)=95.62%\n",
      "[ 2018-04-26 23:36:59,013][kfold_wrapper.log_eval_metrics] Accuracy(layer_13 - estimator_2 - 10_folds.test.predict)=94.70%\n",
      "[ 2018-04-26 23:36:59,014][cascade_classifier.calc_accuracy] Accuracy(layer_13 - train.classifier_average)=95.62%\n",
      "[ 2018-04-26 23:36:59,015][cascade_classifier.calc_accuracy] Accuracy(layer_13 - test.classifier_average)=94.70%\n",
      "[ 2018-04-26 23:36:59,016][cascade_classifier.fit_transform] [Result][Optimal Level Detected] opt_layer_num=11, accuracy_train=95.62%, accuracy_test=94.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(' Time ', '309.195', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "    # X_enc is the concatenated predict_proba result of each estimators of the last layer of the GCForest model\n",
    "    # X_enc.shape =\n",
    "    #   (n_datas, n_estimators * n_classes): If cascade is provided\n",
    "    #   (n_datas, n_estimators * n_classes, dimX, dimY): If only finegrained part is provided\n",
    "    # You can also pass X_test, y_test to fit_transform method, then the accracy on test data will be logged when training.\n",
    "X_train_enc, X_test_enc = gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test)\n",
    "    # WARNING: if you set gc.set_keep_model_in_mem(True), you would have to use\n",
    "    # gc.fit_transform(X_train, y_train, X_test=X_test, y_test=y_test) to evaluate your model.\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2018-04-26 23:36:59,052][cascade_classifier.transform] X_groups_test.shape=[(1263, 57)]\n",
      "[ 2018-04-26 23:36:59,054][cascade_classifier.transform] group_dims=[57]\n",
      "[ 2018-04-26 23:36:59,056][cascade_classifier.transform] X_test.shape=(1263, 57)\n",
      "[ 2018-04-26 23:36:59,058][cascade_classifier.transform] [layer=0] look_indexs=[0], X_cur_test.shape=(1263, 57)\n",
      "[ 2018-04-26 23:37:04,185][cascade_classifier.transform] [layer=1] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:09,167][cascade_classifier.transform] [layer=2] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:13,333][cascade_classifier.transform] [layer=3] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:18,485][cascade_classifier.transform] [layer=4] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:23,544][cascade_classifier.transform] [layer=5] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:28,595][cascade_classifier.transform] [layer=6] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:33,762][cascade_classifier.transform] [layer=7] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:38,848][cascade_classifier.transform] [layer=8] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:44,037][cascade_classifier.transform] [layer=9] look_indexs=[0], X_cur_test.shape=(1263, 63)\n",
      "[ 2018-04-26 23:37:48,973][cascade_classifier.transform] [layer=10] look_indexs=[0], X_cur_test.shape=(1263, 63)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GCForest = 94.615994 %\n"
     ]
    }
   ],
   "source": [
    "# predict\n",
    "y_pred = gc.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy of GCForest = {:.6f} %\".format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 379us/step - loss: 0.6891 - acc: 0.6059\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.6564 - acc: 0.6063\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.5257 - acc: 0.6357\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.4102 - acc: 0.8603\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3213 - acc: 0.9015\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - ETA: 0s - loss: 0.2727 - acc: 0.909 - 0s 66us/step - loss: 0.2648 - acc: 0.9120\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2388 - acc: 0.9151\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 0s 74us/step - loss: 0.2256 - acc: 0.9203\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2190 - acc: 0.9230\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2134 - acc: 0.9249\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2095 - acc: 0.9271\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2078 - acc: 0.9275\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2049 - acc: 0.9245\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2039 - acc: 0.9268\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2019 - acc: 0.9245\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1999 - acc: 0.9256\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1981 - acc: 0.9264\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1973 - acc: 0.9268\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1975 - acc: 0.9279\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1967 - acc: 0.9294\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1948 - acc: 0.9287\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 0s 61us/step - loss: 0.1941 - acc: 0.9279\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 0s 60us/step - loss: 0.1931 - acc: 0.9305\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1934 - acc: 0.9275\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 0s 51us/step - loss: 0.1929 - acc: 0.9298\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 0s 47us/step - loss: 0.1927 - acc: 0.9287\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 0s 55us/step - loss: 0.1925 - acc: 0.9290\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 0s 49us/step - loss: 0.1917 - acc: 0.9305\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 0s 53us/step - loss: 0.1916 - acc: 0.9294\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 0s 57us/step - loss: 0.1902 - acc: 0.9309\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 0s 61us/step - loss: 0.1911 - acc: 0.9305\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 0s 52us/step - loss: 0.1904 - acc: 0.9317\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 0s 60us/step - loss: 0.1903 - acc: 0.9302\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 0s 61us/step - loss: 0.1895 - acc: 0.9320\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1894 - acc: 0.9332\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1893 - acc: 0.9324\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 0s 50us/step - loss: 0.1885 - acc: 0.9309\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1880 - acc: 0.9313\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 0s 60us/step - loss: 0.1876 - acc: 0.9317\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 0s 56us/step - loss: 0.1878 - acc: 0.9317\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 0s 56us/step - loss: 0.1879 - acc: 0.9309\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1872 - acc: 0.9336: 0s - loss: 0.1958 - acc: 0.930 - ETA: 0s - loss: 0.1877 - acc: 0.933\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1869 - acc: 0.9305\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1868 - acc: 0.9309\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 0s 57us/step - loss: 0.1871 - acc: 0.9309\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1867 - acc: 0.9317\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1864 - acc: 0.9302\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1857 - acc: 0.9324\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1861 - acc: 0.9324\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 0s 58us/step - loss: 0.1863 - acc: 0.9313\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1847 - acc: 0.9324\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1859 - acc: 0.9313\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1850 - acc: 0.9320\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1848 - acc: 0.9287\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1849 - acc: 0.9320\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1840 - acc: 0.9332\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1850 - acc: 0.9332\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1836 - acc: 0.9328\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1834 - acc: 0.9339\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1834 - acc: 0.9324\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1839 - acc: 0.9324\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 0s 73us/step - loss: 0.1834 - acc: 0.9317\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1834 - acc: 0.9336\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1829 - acc: 0.9339\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1821 - acc: 0.9362\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1825 - acc: 0.9339\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1823 - acc: 0.9332\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1824 - acc: 0.9328\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1825 - acc: 0.9339\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1819 - acc: 0.9347\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1811 - acc: 0.9324\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1816 - acc: 0.9324\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1805 - acc: 0.9324\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1808 - acc: 0.9343\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1815 - acc: 0.9309\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1831 - acc: 0.9317\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1800 - acc: 0.9343\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1800 - acc: 0.9328\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1804 - acc: 0.9328\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1803 - acc: 0.9320\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1790 - acc: 0.9332\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1798 - acc: 0.9332\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 0s 55us/step - loss: 0.1794 - acc: 0.9347\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1786 - acc: 0.9343\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1793 - acc: 0.9343\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1794 - acc: 0.9332\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1787 - acc: 0.9328\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1784 - acc: 0.9347\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1781 - acc: 0.9336\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1780 - acc: 0.9332\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1775 - acc: 0.9347\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1772 - acc: 0.9343\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1779 - acc: 0.9336\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1776 - acc: 0.9343\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1771 - acc: 0.9347\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1771 - acc: 0.9347\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1784 - acc: 0.9339\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1774 - acc: 0.9347\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1770 - acc: 0.9358\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1774 - acc: 0.9324\n",
      "295/295 [==============================] - 0s 143us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 293us/step - loss: 0.6886 - acc: 0.6116\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.6527 - acc: 0.6116\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.5362 - acc: 0.6116\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.4469 - acc: 0.6606\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.4062 - acc: 0.8784\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3820 - acc: 0.8981\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.3626 - acc: 0.9060\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3471 - acc: 0.9169\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.3331 - acc: 0.9173\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.3205 - acc: 0.9203\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3104 - acc: 0.9196\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.3009 - acc: 0.9230\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.2915 - acc: 0.9260\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2837 - acc: 0.9256\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2773 - acc: 0.9283\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2697 - acc: 0.9283\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2637 - acc: 0.9287\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2587 - acc: 0.9324\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2537 - acc: 0.9302\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2490 - acc: 0.9298\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2441 - acc: 0.9275\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2404 - acc: 0.9287\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2364 - acc: 0.9287\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2327 - acc: 0.9290\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2296 - acc: 0.9290\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2268 - acc: 0.9283\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2240 - acc: 0.9283\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2208 - acc: 0.9298\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2187 - acc: 0.9275\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.2163 - acc: 0.9298\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2135 - acc: 0.9302\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2115 - acc: 0.9294\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2108 - acc: 0.9290\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2083 - acc: 0.9302\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.2066 - acc: 0.9305\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.2046 - acc: 0.9298\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2032 - acc: 0.9320\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.2017 - acc: 0.9309\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2010 - acc: 0.9283\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1986 - acc: 0.9309\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1980 - acc: 0.9290\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1977 - acc: 0.9298\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 0s 63us/step - loss: 0.1957 - acc: 0.9320\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1946 - acc: 0.9305\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1934 - acc: 0.9305\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1929 - acc: 0.9298\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1920 - acc: 0.9324\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1934 - acc: 0.9298\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1905 - acc: 0.9305\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1905 - acc: 0.9317\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1895 - acc: 0.9309\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1890 - acc: 0.9309\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1886 - acc: 0.9313\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1888 - acc: 0.9324\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1869 - acc: 0.9313\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1865 - acc: 0.9317\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1861 - acc: 0.9324\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1865 - acc: 0.9313\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1864 - acc: 0.9336\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1862 - acc: 0.9339\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1847 - acc: 0.9339\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1851 - acc: 0.9320\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1843 - acc: 0.9336\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1841 - acc: 0.9313\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1841 - acc: 0.9328\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1840 - acc: 0.9339\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1841 - acc: 0.9328\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1832 - acc: 0.9313\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1841 - acc: 0.9332\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1829 - acc: 0.9324\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1818 - acc: 0.9324\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1827 - acc: 0.9336\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1828 - acc: 0.9328\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1815 - acc: 0.9324\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1828 - acc: 0.9328\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1819 - acc: 0.9328\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1826 - acc: 0.9336\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1818 - acc: 0.9332\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1815 - acc: 0.9324\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1811 - acc: 0.9328\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1812 - acc: 0.9347\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1808 - acc: 0.9324\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1806 - acc: 0.9347\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1806 - acc: 0.9328\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1804 - acc: 0.9343\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1809 - acc: 0.9324\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1800 - acc: 0.9324\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1798 - acc: 0.9328\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1796 - acc: 0.9366\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1804 - acc: 0.9351\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1801 - acc: 0.9332\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1795 - acc: 0.9343\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1802 - acc: 0.9332\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1792 - acc: 0.9324\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1793 - acc: 0.9347\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1789 - acc: 0.9336\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1789 - acc: 0.9347\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1787 - acc: 0.9347\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1782 - acc: 0.9354\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1781 - acc: 0.9336\n",
      "295/295 [==============================] - 0s 218us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 278us/step - loss: 0.6895 - acc: 0.5991\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.6685 - acc: 0.6100\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.5774 - acc: 0.6100\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.4826 - acc: 0.6100\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.4339 - acc: 0.7603\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.4041 - acc: 0.8796\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3820 - acc: 0.8935\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3643 - acc: 0.9026\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.3483 - acc: 0.9094\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.3355 - acc: 0.9143\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.3227 - acc: 0.9192\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.3115 - acc: 0.9158\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.3026 - acc: 0.9181\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2936 - acc: 0.9222\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2854 - acc: 0.9211\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2783 - acc: 0.9241\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2719 - acc: 0.9245\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2657 - acc: 0.9256\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2593 - acc: 0.9264\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2542 - acc: 0.9271\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2492 - acc: 0.9260\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2453 - acc: 0.9271\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2407 - acc: 0.9271\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2379 - acc: 0.9260\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2336 - acc: 0.9283\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2309 - acc: 0.9275\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2279 - acc: 0.9268\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.2247 - acc: 0.9275\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2223 - acc: 0.9279\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2199 - acc: 0.9290\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2171 - acc: 0.9287\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.2152 - acc: 0.9290\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2145 - acc: 0.9287\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2111 - acc: 0.9290\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2099 - acc: 0.9287\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2078 - acc: 0.9313\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2076 - acc: 0.9305\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.2059 - acc: 0.9294\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2040 - acc: 0.9298\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2040 - acc: 0.9298\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2022 - acc: 0.9302\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2006 - acc: 0.9302\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1994 - acc: 0.9305\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1981 - acc: 0.9287\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1975 - acc: 0.9313\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 0s 61us/step - loss: 0.1965 - acc: 0.9302\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1956 - acc: 0.9302\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1954 - acc: 0.9313\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1937 - acc: 0.9328\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1934 - acc: 0.9320\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1922 - acc: 0.9320\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1920 - acc: 0.9302\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1906 - acc: 0.9328\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1896 - acc: 0.9336\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1919 - acc: 0.9302\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1895 - acc: 0.9320\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1892 - acc: 0.9317\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1885 - acc: 0.9313\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1875 - acc: 0.9332\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1878 - acc: 0.9343\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1870 - acc: 0.9324\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1861 - acc: 0.9332\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1854 - acc: 0.9324\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1859 - acc: 0.9324\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1847 - acc: 0.9328\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1839 - acc: 0.9328\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1839 - acc: 0.9320\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1833 - acc: 0.9351\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1826 - acc: 0.9328\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1827 - acc: 0.9317\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1827 - acc: 0.9332\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1824 - acc: 0.9332\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1816 - acc: 0.9339\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1818 - acc: 0.9351\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1819 - acc: 0.9332\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1809 - acc: 0.9347\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1810 - acc: 0.9351\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1814 - acc: 0.9332\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1820 - acc: 0.9366\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1811 - acc: 0.9343\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1804 - acc: 0.9343\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1798 - acc: 0.9339\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1791 - acc: 0.9354\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1804 - acc: 0.9347\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1808 - acc: 0.9362\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1793 - acc: 0.9339\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1804 - acc: 0.9324\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1793 - acc: 0.9343\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1791 - acc: 0.9354\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1784 - acc: 0.9324\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1799 - acc: 0.9351\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1800 - acc: 0.9339\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1794 - acc: 0.9354\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1802 - acc: 0.9343\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1792 - acc: 0.9343\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1792 - acc: 0.9336\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1791 - acc: 0.9328\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1786 - acc: 0.9339\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.1788 - acc: 0.9373\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.1779 - acc: 0.9358\n",
      "295/295 [==============================] - 0s 276us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 230us/step - loss: 0.6894 - acc: 0.6040\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.6544 - acc: 0.6051\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.4870 - acc: 0.7875\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.3181 - acc: 0.8935\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.2563 - acc: 0.9117\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2372 - acc: 0.9173\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2265 - acc: 0.9188\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.2183 - acc: 0.9203\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.2141 - acc: 0.9230\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2104 - acc: 0.9245\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.2064 - acc: 0.9283\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2027 - acc: 0.9283\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.2010 - acc: 0.9283\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1997 - acc: 0.9290\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1977 - acc: 0.9294\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1945 - acc: 0.9298\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1957 - acc: 0.9290\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1948 - acc: 0.9298\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1934 - acc: 0.9309\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1924 - acc: 0.9302\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1901 - acc: 0.9302\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1907 - acc: 0.9294\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1897 - acc: 0.9328\n",
      "Epoch 24/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1890 - acc: 0.9309\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1887 - acc: 0.9309\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1883 - acc: 0.9287\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1876 - acc: 0.9305\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1873 - acc: 0.9317\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1859 - acc: 0.9313\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1854 - acc: 0.9324\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1862 - acc: 0.9309\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1846 - acc: 0.9324\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1846 - acc: 0.9317\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1840 - acc: 0.9317\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1830 - acc: 0.9298\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 0s 58us/step - loss: 0.1825 - acc: 0.9324\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1829 - acc: 0.9332\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1813 - acc: 0.9328\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1825 - acc: 0.9320\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1808 - acc: 0.9305\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.1830 - acc: 0.9336\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1802 - acc: 0.9324\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1791 - acc: 0.9320\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1790 - acc: 0.9339\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1786 - acc: 0.9317\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.1780 - acc: 0.9328\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1777 - acc: 0.9351\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 0s 72us/step - loss: 0.1772 - acc: 0.9336\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1773 - acc: 0.9313\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1764 - acc: 0.9328\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1771 - acc: 0.9354\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1768 - acc: 0.9324\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1756 - acc: 0.9339\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1757 - acc: 0.9313\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1758 - acc: 0.9328\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1754 - acc: 0.9328\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1740 - acc: 0.9354\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1750 - acc: 0.9347\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1739 - acc: 0.9339\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1738 - acc: 0.9339\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1739 - acc: 0.9358\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1734 - acc: 0.9362\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1721 - acc: 0.9370\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1720 - acc: 0.9354\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1721 - acc: 0.9366\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1713 - acc: 0.9347\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1703 - acc: 0.9381\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1701 - acc: 0.9370\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1696 - acc: 0.9370\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 0s 73us/step - loss: 0.1698 - acc: 0.9351\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1693 - acc: 0.9377\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1689 - acc: 0.9343\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 0s 66us/step - loss: 0.1681 - acc: 0.9377\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 0s 67us/step - loss: 0.1679 - acc: 0.9385\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1681 - acc: 0.9381\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 0s 71us/step - loss: 0.1678 - acc: 0.9385\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1673 - acc: 0.9354\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1666 - acc: 0.9377\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1670 - acc: 0.9396\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1674 - acc: 0.9388\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1659 - acc: 0.9392\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 0s 64us/step - loss: 0.1663 - acc: 0.9392\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1667 - acc: 0.9362\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 0s 65us/step - loss: 0.1656 - acc: 0.9377\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1642 - acc: 0.9377\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1653 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1647 - acc: 0.9377\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1638 - acc: 0.9377\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1636 - acc: 0.9362\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 0s 62us/step - loss: 0.1631 - acc: 0.9388\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1624 - acc: 0.9373\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1617 - acc: 0.9385\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 0s 68us/step - loss: 0.1624 - acc: 0.9381\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1617 - acc: 0.9370\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1606 - acc: 0.9407\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1602 - acc: 0.9404\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1601 - acc: 0.9385\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1604 - acc: 0.9370\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 0s 69us/step - loss: 0.1591 - acc: 0.9385\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 0s 70us/step - loss: 0.1582 - acc: 0.9407\n",
      "295/295 [==============================] - 0s 300us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.6894 - acc: 0.6053\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6620 - acc: 0.6072\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.5356 - acc: 0.6072\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.4421 - acc: 0.6351\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.4036 - acc: 0.8898\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.3784 - acc: 0.9049\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.3588 - acc: 0.9102\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.3421 - acc: 0.9185\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.3277 - acc: 0.9208\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.3153 - acc: 0.9230\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.3048 - acc: 0.9257\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2938 - acc: 0.9291\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2866 - acc: 0.9264\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2772 - acc: 0.9279\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2698 - acc: 0.9294\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2639 - acc: 0.9283\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2576 - acc: 0.9325\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2526 - acc: 0.9294\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.2471 - acc: 0.9309\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.2431 - acc: 0.9325\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.2377 - acc: 0.9321\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.2347 - acc: 0.9291\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2318 - acc: 0.9325\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2270 - acc: 0.9347\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2242 - acc: 0.9328\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2207 - acc: 0.9309\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2177 - acc: 0.9321\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2148 - acc: 0.9302\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2137 - acc: 0.9302\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2109 - acc: 0.9325\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2083 - acc: 0.9321\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2066 - acc: 0.9317\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2043 - acc: 0.9332\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2026 - acc: 0.9313\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.2009 - acc: 0.9336\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2003 - acc: 0.9287\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1983 - acc: 0.9336\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.1969 - acc: 0.9321\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1952 - acc: 0.9336\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1932 - acc: 0.9347\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1917 - acc: 0.9328\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1930 - acc: 0.9321\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1910 - acc: 0.9321\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1889 - acc: 0.9313\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1877 - acc: 0.9336\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 76us/step - loss: 0.1881 - acc: 0.9332\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1880 - acc: 0.9309\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1865 - acc: 0.9328\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1871 - acc: 0.9328\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1852 - acc: 0.9336\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1839 - acc: 0.9343\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1832 - acc: 0.9317\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1823 - acc: 0.9336\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1824 - acc: 0.9336\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1814 - acc: 0.9325\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1817 - acc: 0.9336\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1822 - acc: 0.9336\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1819 - acc: 0.9347\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1811 - acc: 0.9336\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1791 - acc: 0.9340\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1791 - acc: 0.9340\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1798 - acc: 0.9351\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1780 - acc: 0.9325\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1788 - acc: 0.9332\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1780 - acc: 0.9321\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1774 - acc: 0.9336\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1784 - acc: 0.9355\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1771 - acc: 0.9351\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.1772 - acc: 0.9328\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1767 - acc: 0.9358\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1767 - acc: 0.9355\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1763 - acc: 0.9351\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1758 - acc: 0.9355\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1760 - acc: 0.9343\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1751 - acc: 0.9336\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1756 - acc: 0.9362\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1745 - acc: 0.9377\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1741 - acc: 0.9370\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1751 - acc: 0.9343\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1742 - acc: 0.9347\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1764 - acc: 0.9347\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1758 - acc: 0.9377\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1736 - acc: 0.9362\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1739 - acc: 0.9370\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1757 - acc: 0.9358\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1737 - acc: 0.9366\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1734 - acc: 0.9366\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1721 - acc: 0.9362\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1735 - acc: 0.9362\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1738 - acc: 0.9351\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1728 - acc: 0.9351\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1734 - acc: 0.9370\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1741 - acc: 0.9362\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1719 - acc: 0.9377\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1724 - acc: 0.9362\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1714 - acc: 0.9355\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1723 - acc: 0.9347\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1730 - acc: 0.9347\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1716 - acc: 0.9366\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1713 - acc: 0.9374\n",
      "294/294 [==============================] - 0s 391us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.6890 - acc: 0.6079\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6503 - acc: 0.6075\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.5173 - acc: 0.6075\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.4221 - acc: 0.8574\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.3562 - acc: 0.8958\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2972 - acc: 0.9136\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2556 - acc: 0.9181\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2314 - acc: 0.9211\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.2179 - acc: 0.9242\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2095 - acc: 0.9257\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.2046 - acc: 0.9268\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2007 - acc: 0.9287\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1976 - acc: 0.9279\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1960 - acc: 0.9309\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1949 - acc: 0.9294\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1932 - acc: 0.9298\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1907 - acc: 0.9309\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1889 - acc: 0.9306\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1882 - acc: 0.9317\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1867 - acc: 0.9317\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1868 - acc: 0.9298\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1855 - acc: 0.9321\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1840 - acc: 0.9347\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1836 - acc: 0.9336\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1825 - acc: 0.9306\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1821 - acc: 0.9328\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1810 - acc: 0.9321\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1806 - acc: 0.9325\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1799 - acc: 0.9332\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1793 - acc: 0.9336\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1788 - acc: 0.9317\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1793 - acc: 0.9355\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1780 - acc: 0.9336\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1784 - acc: 0.9343\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1782 - acc: 0.9347\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1775 - acc: 0.9340\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1770 - acc: 0.9370\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1772 - acc: 0.9328\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1766 - acc: 0.9347\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1763 - acc: 0.9355\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1761 - acc: 0.9347\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1755 - acc: 0.9325\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1749 - acc: 0.9355\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1748 - acc: 0.9351\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1749 - acc: 0.9366\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1744 - acc: 0.9362\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1750 - acc: 0.9343\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1741 - acc: 0.9347\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1735 - acc: 0.9362\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1730 - acc: 0.9377\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1738 - acc: 0.9358\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1743 - acc: 0.9362\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1733 - acc: 0.9370\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1723 - acc: 0.9377\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1718 - acc: 0.9374\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1725 - acc: 0.9347\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1711 - acc: 0.9381\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1719 - acc: 0.9381\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1712 - acc: 0.9332\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1717 - acc: 0.9381\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1706 - acc: 0.9355\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1714 - acc: 0.9366\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.1649 - acc: 0.939 - 0s 66us/step - loss: 0.1704 - acc: 0.9358\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1694 - acc: 0.9389\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1694 - acc: 0.9366\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1693 - acc: 0.9374\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1691 - acc: 0.9370\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1684 - acc: 0.9396\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1696 - acc: 0.9355\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1698 - acc: 0.9366\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1678 - acc: 0.9396\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1681 - acc: 0.9374\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1698 - acc: 0.9385\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1674 - acc: 0.9392\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1672 - acc: 0.9392\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1674 - acc: 0.9389\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1662 - acc: 0.9385\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1665 - acc: 0.9389\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1678 - acc: 0.9415\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1675 - acc: 0.9408\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1658 - acc: 0.9389\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1656 - acc: 0.9392\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1657 - acc: 0.9404\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1647 - acc: 0.9389\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1654 - acc: 0.9370\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1646 - acc: 0.9400\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1652 - acc: 0.9404\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1642 - acc: 0.9408\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1641 - acc: 0.9392\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1629 - acc: 0.9408\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1627 - acc: 0.9411\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1618 - acc: 0.9411\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1613 - acc: 0.9426\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1611 - acc: 0.9400\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1606 - acc: 0.9419\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1606 - acc: 0.9404\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1598 - acc: 0.9434\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1598 - acc: 0.9377\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1592 - acc: 0.9419\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1572 - acc: 0.9392\n",
      "294/294 [==============================] - 0s 427us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.6906 - acc: 0.6026\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6858 - acc: 0.6008\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6821 - acc: 0.6008\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6794 - acc: 0.6008\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6773 - acc: 0.6008\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6759 - acc: 0.6008\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6749 - acc: 0.6008\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6742 - acc: 0.6008\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6737 - acc: 0.6008\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6734 - acc: 0.6008\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6732 - acc: 0.6008\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6730 - acc: 0.6008\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6729 - acc: 0.6008\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 58us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 62us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 62us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 61us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 61us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 49/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6728 - acc: 0.6008\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 75us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6727 - acc: 0.6008\n",
      "294/294 [==============================] - 0s 494us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 264us/step - loss: 0.6904 - acc: 0.5977\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6846 - acc: 0.6121\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6801 - acc: 0.6121\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6767 - acc: 0.6121\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6741 - acc: 0.6121\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6723 - acc: 0.6121\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6709 - acc: 0.6121\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6700 - acc: 0.6121\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6693 - acc: 0.6121\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6688 - acc: 0.6121\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6685 - acc: 0.6121\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6683 - acc: 0.6121\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6681 - acc: 0.6121\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6681 - acc: 0.6121\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6680 - acc: 0.6121\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 77us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6678 - acc: 0.6121\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.6679 - acc: 0.6121\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.6678 - acc: 0.6121\n",
      "294/294 [==============================] - 0s 571us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 352us/step - loss: 0.6899 - acc: 0.6091\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.6670 - acc: 0.6106\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.5500 - acc: 0.6106\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.4541 - acc: 0.6106\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.4130 - acc: 0.8592\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.3873 - acc: 0.8970\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.3674 - acc: 0.9038\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.3500 - acc: 0.9117\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.3359 - acc: 0.9177\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.3229 - acc: 0.9204\n",
      "Epoch 11/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.3109 - acc: 0.9208\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.3005 - acc: 0.9245\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2920 - acc: 0.9264\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2844 - acc: 0.9279\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2761 - acc: 0.9291\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2692 - acc: 0.9302\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2635 - acc: 0.9275\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.2578 - acc: 0.9279\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.2530 - acc: 0.9279\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2481 - acc: 0.9298\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2435 - acc: 0.9321\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2403 - acc: 0.9298\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2370 - acc: 0.9309\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2329 - acc: 0.9309\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.2301 - acc: 0.9306\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2273 - acc: 0.9309\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.2242 - acc: 0.9325\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2219 - acc: 0.9325\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2198 - acc: 0.9332\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2177 - acc: 0.9317\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2150 - acc: 0.9317\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2135 - acc: 0.9317\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2120 - acc: 0.9302\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.2091 - acc: 0.9298\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2070 - acc: 0.9313\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2063 - acc: 0.9321\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.2038 - acc: 0.9325\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.2026 - acc: 0.9313\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2016 - acc: 0.9317\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.2000 - acc: 0.9343\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1989 - acc: 0.9321\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1984 - acc: 0.9336\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1970 - acc: 0.9325\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1960 - acc: 0.9309\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1947 - acc: 0.9321\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1941 - acc: 0.9347\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1929 - acc: 0.9321\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1920 - acc: 0.9321\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1914 - acc: 0.9343\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1909 - acc: 0.9332\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1907 - acc: 0.9328\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1896 - acc: 0.9340\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1900 - acc: 0.9321\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1893 - acc: 0.9332\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1888 - acc: 0.9336\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1884 - acc: 0.9355\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1872 - acc: 0.9362\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1875 - acc: 0.9351\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1860 - acc: 0.9343\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1866 - acc: 0.9343\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1862 - acc: 0.9347\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1858 - acc: 0.9355\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1860 - acc: 0.9340\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1853 - acc: 0.9358\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1856 - acc: 0.9366\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1837 - acc: 0.9358\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1843 - acc: 0.9340\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1846 - acc: 0.9343\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1844 - acc: 0.9351\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1833 - acc: 0.9340\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1830 - acc: 0.9374\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1828 - acc: 0.9374\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1822 - acc: 0.9347\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1825 - acc: 0.9362\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1825 - acc: 0.9358\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1818 - acc: 0.9366\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1819 - acc: 0.9362\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1808 - acc: 0.9374\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1814 - acc: 0.9377\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1808 - acc: 0.9370\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.1805 - acc: 0.9340\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1819 - acc: 0.9366\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1810 - acc: 0.9374\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1802 - acc: 0.9370\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1792 - acc: 0.9377\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1805 - acc: 0.9377\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1794 - acc: 0.9355\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1797 - acc: 0.9362\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1789 - acc: 0.9355\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1784 - acc: 0.9370\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1789 - acc: 0.9351\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1780 - acc: 0.9381\n",
      "Epoch 93/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1777 - acc: 0.9381\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1776 - acc: 0.9404\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1780 - acc: 0.9389\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 66us/step - loss: 0.1778 - acc: 0.9396\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.1775 - acc: 0.9389\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1768 - acc: 0.9381\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1763 - acc: 0.9392\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1773 - acc: 0.9370\n",
      "294/294 [==============================] - 0s 569us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 287us/step - loss: 0.6889 - acc: 0.6079\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.6433 - acc: 0.6091\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.4931 - acc: 0.7226\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.3648 - acc: 0.8879\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2818 - acc: 0.9109\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.2438 - acc: 0.9143\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2268 - acc: 0.9192\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.2179 - acc: 0.9192\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.2130 - acc: 0.9226\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.2092 - acc: 0.9242\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.2055 - acc: 0.9283\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.2035 - acc: 0.9264\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.2007 - acc: 0.9287\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 0s 77us/step - loss: 0.1993 - acc: 0.9291\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1982 - acc: 0.9309\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 0s 76us/step - loss: 0.1981 - acc: 0.9287\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1948 - acc: 0.9306\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1953 - acc: 0.9321\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1927 - acc: 0.9321\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 0s 75us/step - loss: 0.1921 - acc: 0.9343\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1906 - acc: 0.9340\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1905 - acc: 0.9325\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1905 - acc: 0.9340\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1892 - acc: 0.9332\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1885 - acc: 0.9332\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1878 - acc: 0.9328\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1876 - acc: 0.9321\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1879 - acc: 0.9332\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 0s 62us/step - loss: 0.1865 - acc: 0.9336\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 0s 53us/step - loss: 0.1870 - acc: 0.9347\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 0s 53us/step - loss: 0.1860 - acc: 0.9328\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 0s 57us/step - loss: 0.1861 - acc: 0.9328\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 0s 58us/step - loss: 0.1851 - acc: 0.9347\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 0s 52us/step - loss: 0.1846 - acc: 0.9347\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 0s 50us/step - loss: 0.1834 - acc: 0.9340\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 0s 49us/step - loss: 0.1833 - acc: 0.9328\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 0s 52us/step - loss: 0.1837 - acc: 0.9355\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 0s 61us/step - loss: 0.1827 - acc: 0.9343\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 0s 64us/step - loss: 0.1824 - acc: 0.9347\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1817 - acc: 0.9336\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1820 - acc: 0.9313\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1817 - acc: 0.9370\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1809 - acc: 0.9343\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 56us/step - loss: 0.1812 - acc: 0.9358\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 56us/step - loss: 0.1803 - acc: 0.9336\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 61us/step - loss: 0.1798 - acc: 0.9355\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 0s 57us/step - loss: 0.1810 - acc: 0.9347\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 0s 61us/step - loss: 0.1806 - acc: 0.9358\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.1793 - acc: 0.9347\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 0s 63us/step - loss: 0.1792 - acc: 0.9381\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 0s 58us/step - loss: 0.1784 - acc: 0.9332\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 0s 52us/step - loss: 0.1787 - acc: 0.9328\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1781 - acc: 0.9340\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1765 - acc: 0.9336\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1772 - acc: 0.9343\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1763 - acc: 0.9340\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1748 - acc: 0.9358\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 0s 75us/step - loss: 0.1752 - acc: 0.9347\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1759 - acc: 0.9340\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1757 - acc: 0.9351\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1747 - acc: 0.9358\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1745 - acc: 0.9343\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1746 - acc: 0.9336\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1734 - acc: 0.9358\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1730 - acc: 0.9355\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1732 - acc: 0.9366\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1725 - acc: 0.9366\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1727 - acc: 0.9351\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1713 - acc: 0.9370\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1704 - acc: 0.9370\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1729 - acc: 0.9370\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1722 - acc: 0.9374\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 0s 67us/step - loss: 0.1722 - acc: 0.9351\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1704 - acc: 0.9385\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1698 - acc: 0.9381\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 69us/step - loss: 0.1692 - acc: 0.9362\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 0s 65us/step - loss: 0.1699 - acc: 0.9370\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1698 - acc: 0.9362\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1697 - acc: 0.9381\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1692 - acc: 0.9392\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1682 - acc: 0.9400\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1683 - acc: 0.9408\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1676 - acc: 0.9377\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1689 - acc: 0.9377\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1677 - acc: 0.9381\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1673 - acc: 0.9392\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 0s 70us/step - loss: 0.1677 - acc: 0.9392\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1674 - acc: 0.9370\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1675 - acc: 0.9389\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1663 - acc: 0.9396\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 0s 68us/step - loss: 0.1673 - acc: 0.9392\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1658 - acc: 0.9389\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1655 - acc: 0.9408\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 0s 75us/step - loss: 0.1658 - acc: 0.9389\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1665 - acc: 0.9389\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 0s 72us/step - loss: 0.1656 - acc: 0.9408\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 71us/step - loss: 0.1648 - acc: 0.9415\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 74us/step - loss: 0.1647 - acc: 0.9404\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1642 - acc: 0.9404: 0s - loss: 0.1764 - acc: 0.93\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 0s 73us/step - loss: 0.1642 - acc: 0.9396\n",
      "294/294 [==============================] - 0s 648us/step\n",
      "Accuracy mean: 0.867436873135\n",
      "Accuracy variance: 0.124891770873\n",
      "(' Time ', '199.785', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "# estimators \n",
    "# Evaluating the ANN\n",
    "t0 = time()\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    classifier.add(Dense(units = 4, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 364us/step - loss: 0.3556 - acc: 0.8747\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 1s 208us/step - loss: 0.2079 - acc: 0.9234\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.1888 - acc: 0.9309\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.1754 - acc: 0.9362\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 1s 491us/step - loss: 0.1696 - acc: 0.9328\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.1684 - acc: 0.9343\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 1s 245us/step - loss: 0.1535 - acc: 0.9430\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 1s 236us/step - loss: 0.1395 - acc: 0.9441\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 1s 204us/step - loss: 0.1333 - acc: 0.9505\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.1203 - acc: 0.9532\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.1137 - acc: 0.9581\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.1080 - acc: 0.9596 0s - loss: 0.0999 - ac\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 2s 615us/step - loss: 0.0983 - acc: 0.9626\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 1s 303us/step - loss: 0.0911 - acc: 0.9675\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0959 - acc: 0.9638\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 1s 229us/step - loss: 0.0826 - acc: 0.9668\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 1s 227us/step - loss: 0.0944 - acc: 0.9653\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0783 - acc: 0.9690\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0744 - acc: 0.9728\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0732 - acc: 0.9713\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 1s 218us/step - loss: 0.0682 - acc: 0.9724\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0623 - acc: 0.9777\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0536 - acc: 0.9819\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0533 - acc: 0.9792\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0534 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 1s 230us/step - loss: 0.0578 - acc: 0.9766\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 1s 232us/step - loss: 0.0473 - acc: 0.9807\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0530 - acc: 0.9796\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0455 - acc: 0.9838\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 1s 232us/step - loss: 0.0490 - acc: 0.9811\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 1s 218us/step - loss: 0.0358 - acc: 0.9872\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0386 - acc: 0.9883\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0326 - acc: 0.9894\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0332 - acc: 0.9887\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0444 - acc: 0.9845\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0331 - acc: 0.9872\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0302 - acc: 0.9894\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0332 - acc: 0.9879\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0322 - acc: 0.9902\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0249 - acc: 0.9909\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0262 - acc: 0.9913\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0286 - acc: 0.9898\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0244 - acc: 0.9917\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0490 - acc: 0.9811\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 1s 218us/step - loss: 0.0380 - acc: 0.9864\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 1s 218us/step - loss: 0.0262 - acc: 0.9902\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.0221 - acc: 0.9932\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0200 - acc: 0.9947\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0192 - acc: 0.9932\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0214 - acc: 0.9932\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0172 - acc: 0.9947\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0206 - acc: 0.9936\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 1s 223us/step - loss: 0.0316 - acc: 0.9883\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 1s 229us/step - loss: 0.0262 - acc: 0.9924\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0239 - acc: 0.9917\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.0206 - acc: 0.9936\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0168 - acc: 0.9947\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.0212 - acc: 0.9906\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 1s 226us/step - loss: 0.0239 - acc: 0.9928\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.0182 - acc: 0.9943\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0176 - acc: 0.9940\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0194 - acc: 0.9936\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0172 - acc: 0.9940\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.0224 - acc: 0.9913\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0180 - acc: 0.9943\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0175 - acc: 0.9936\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0180 - acc: 0.9962\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0243 - acc: 0.9921\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0448 - acc: 0.9868\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0233 - acc: 0.9932\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0190 - acc: 0.9928\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 1s 190us/step - loss: 0.0153 - acc: 0.9943\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0142 - acc: 0.9955\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 1s 192us/step - loss: 0.0137 - acc: 0.9951\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0179 - acc: 0.9951\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 1s 230us/step - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0170 - acc: 0.9936\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0194 - acc: 0.9947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0379 - acc: 0.9875\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0191 - acc: 0.9932\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.0154 - acc: 0.9947\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0230 - acc: 0.9921\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.0174 - acc: 0.9940\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0139 - acc: 0.9951\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0130 - acc: 0.9958\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.0165 - acc: 0.9932\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.0165 - acc: 0.9940\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0172 - acc: 0.9947\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0175 - acc: 0.9940\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0174 - acc: 0.9940\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0139 - acc: 0.9962\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0159 - acc: 0.9947\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0129 - acc: 0.9958\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0123 - acc: 0.9947\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0121 - acc: 0.9955\n",
      "295/295 [==============================] - 0s 146us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 351us/step - loss: 0.3445 - acc: 0.8649\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.2030 - acc: 0.9298\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 1s 219us/step - loss: 0.1868 - acc: 0.9324\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 1s 232us/step - loss: 0.1753 - acc: 0.9339\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.1657 - acc: 0.9396\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.1539 - acc: 0.9430\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.1433 - acc: 0.9468\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.1360 - acc: 0.9494\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.1347 - acc: 0.9487\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.1189 - acc: 0.9570\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.1185 - acc: 0.9536\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.1023 - acc: 0.9645\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.0973 - acc: 0.9638\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0932 - acc: 0.9645\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0866 - acc: 0.9687\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0850 - acc: 0.9694\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0771 - acc: 0.9721\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0726 - acc: 0.9740\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0704 - acc: 0.9758\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0609 - acc: 0.9781\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0549 - acc: 0.9800\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0510 - acc: 0.9823\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0592 - acc: 0.9758\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0525 - acc: 0.9815\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0538 - acc: 0.9804\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 1s 226us/step - loss: 0.0500 - acc: 0.9819\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0458 - acc: 0.9826\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.0417 - acc: 0.9849\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0448 - acc: 0.9834\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0385 - acc: 0.9883\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0394 - acc: 0.9864\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0317 - acc: 0.9902\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0346 - acc: 0.9875\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0340 - acc: 0.9894\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0291 - acc: 0.9906\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.0264 - acc: 0.9928\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 1s 197us/step - loss: 0.0281 - acc: 0.9902\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 1s 208us/step - loss: 0.0251 - acc: 0.9924\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0237 - acc: 0.9928\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0227 - acc: 0.9936\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0283 - acc: 0.9898\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0328 - acc: 0.9921\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0301 - acc: 0.9909\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0219 - acc: 0.9932\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0282 - acc: 0.9913\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.0281 - acc: 0.9894\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0341 - acc: 0.9875\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0245 - acc: 0.9913\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0199 - acc: 0.9940\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 1s 208us/step - loss: 0.0200 - acc: 0.9936\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 1s 226us/step - loss: 0.0201 - acc: 0.9932\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0221 - acc: 0.9921\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0298 - acc: 0.9906\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0215 - acc: 0.9921\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0251 - acc: 0.9898\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0280 - acc: 0.9906\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0268 - acc: 0.9913\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0321 - acc: 0.9887 0s - loss: 0.0202 - ac\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0253 - acc: 0.9917\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.0177 - acc: 0.9940\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 1s 190us/step - loss: 0.0201 - acc: 0.9928\n",
      "Epoch 62/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0215 - acc: 0.9924\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0193 - acc: 0.9932\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 1s 226us/step - loss: 0.0200 - acc: 0.9921\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0184 - acc: 0.9936\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.0144 - acc: 0.9955\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 1s 203us/step - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 1s 198us/step - loss: 0.0153 - acc: 0.9947\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0141 - acc: 0.9958\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0141 - acc: 0.9951\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0260 - acc: 0.9906\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0348 - acc: 0.9906\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0325 - acc: 0.9887\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 1s 218us/step - loss: 0.0241 - acc: 0.9928\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0152 - acc: 0.9951\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 0s 182us/step - loss: 0.0144 - acc: 0.9951\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0162 - acc: 0.9955\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 0s 187us/step - loss: 0.0145 - acc: 0.9955\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 0s 176us/step - loss: 0.0135 - acc: 0.9958\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 1s 214us/step - loss: 0.0142 - acc: 0.9940\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.0186 - acc: 0.9928\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0327 - acc: 0.9875\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 0s 174us/step - loss: 0.0273 - acc: 0.9883\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 0s 154us/step - loss: 0.0193 - acc: 0.9943\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 1s 197us/step - loss: 0.0172 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 0s 179us/step - loss: 0.0176 - acc: 0.9936\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 1s 204us/step - loss: 0.0177 - acc: 0.9936\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 0s 186us/step - loss: 0.0147 - acc: 0.9958\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.0143 - acc: 0.9958\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0137 - acc: 0.9958\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 1s 279us/step - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 1s 311us/step - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 1s 265us/step - loss: 0.0200 - acc: 0.9917\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0144 - acc: 0.9951\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 0s 185us/step - loss: 0.0125 - acc: 0.9958\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 1s 190us/step - loss: 0.0136 - acc: 0.9940\n",
      "295/295 [==============================] - 0s 198us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 343us/step - loss: 0.3464 - acc: 0.8445\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.2042 - acc: 0.9287\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.1883 - acc: 0.9283\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.1712 - acc: 0.9385\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 1s 191us/step - loss: 0.1687 - acc: 0.9385\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.1506 - acc: 0.9430\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 1s 189us/step - loss: 0.1450 - acc: 0.9475\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 1s 204us/step - loss: 0.1455 - acc: 0.9445\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.1342 - acc: 0.9498\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.1204 - acc: 0.9566\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 0s 185us/step - loss: 0.1125 - acc: 0.9581\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 1s 210us/step - loss: 0.1053 - acc: 0.9600\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 1s 197us/step - loss: 0.1063 - acc: 0.9626\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.0924 - acc: 0.9687\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 0s 172us/step - loss: 0.0844 - acc: 0.9721\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 0s 181us/step - loss: 0.0822 - acc: 0.9698\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 1s 223us/step - loss: 0.0742 - acc: 0.9732\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 1s 199us/step - loss: 0.0749 - acc: 0.9709\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 0s 184us/step - loss: 0.0678 - acc: 0.9770\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0632 - acc: 0.9777\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0572 - acc: 0.9796\n",
      "Epoch 22/100\n",
      "2649/2649 [==============================] - 1s 192us/step - loss: 0.0594 - acc: 0.9773\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 1s 202us/step - loss: 0.0568 - acc: 0.9796\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 0s 188us/step - loss: 0.0492 - acc: 0.9845\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0494 - acc: 0.9826\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 0s 168us/step - loss: 0.0467 - acc: 0.9841\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 1s 208us/step - loss: 0.0373 - acc: 0.9902\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 1s 191us/step - loss: 0.0395 - acc: 0.9857\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 1s 211us/step - loss: 0.0346 - acc: 0.9906\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 1s 196us/step - loss: 0.0327 - acc: 0.9887\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 0s 184us/step - loss: 0.0398 - acc: 0.9864\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 0s 166us/step - loss: 0.0343 - acc: 0.9894\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 0s 166us/step - loss: 0.0301 - acc: 0.9909\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 1s 204us/step - loss: 0.0291 - acc: 0.9928\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 0s 183us/step - loss: 0.0276 - acc: 0.9909\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 0s 185us/step - loss: 0.0307 - acc: 0.9891\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 0s 178us/step - loss: 0.0264 - acc: 0.9921\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 0s 178us/step - loss: 0.0260 - acc: 0.9928\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 0s 180us/step - loss: 0.0220 - acc: 0.9924\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 0s 188us/step - loss: 0.0231 - acc: 0.9924\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 1s 274us/step - loss: 0.0288 - acc: 0.9928\n",
      "Epoch 42/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649/2649 [==============================] - 1s 263us/step - loss: 0.0244 - acc: 0.9932\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 1s 346us/step - loss: 0.0450 - acc: 0.9853\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 1s 471us/step - loss: 0.0373 - acc: 0.9872\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 1s 234us/step - loss: 0.0251 - acc: 0.9947\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0197 - acc: 0.9943\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 0s 188us/step - loss: 0.0191 - acc: 0.9936\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0176 - acc: 0.9947\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0182 - acc: 0.9932\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 0s 188us/step - loss: 0.0172 - acc: 0.9943\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 0s 174us/step - loss: 0.0201 - acc: 0.9943\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 1s 217us/step - loss: 0.0315 - acc: 0.9894\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 0s 188us/step - loss: 0.0407 - acc: 0.9864\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 1s 193us/step - loss: 0.0204 - acc: 0.9955\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0189 - acc: 0.9940\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 1s 245us/step - loss: 0.0176 - acc: 0.9955\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 1s 270us/step - loss: 0.0170 - acc: 0.9958\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 1s 254us/step - loss: 0.0209 - acc: 0.9928\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 0s 183us/step - loss: 0.0159 - acc: 0.9955\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 1s 246us/step - loss: 0.0149 - acc: 0.9943\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 1s 301us/step - loss: 0.0168 - acc: 0.9951\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 1s 341us/step - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 1s 299us/step - loss: 0.0159 - acc: 0.9951\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 1s 346us/step - loss: 0.0319 - acc: 0.9898\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 1s 281us/step - loss: 0.0427 - acc: 0.9875\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 1s 348us/step - loss: 0.0250 - acc: 0.9924\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 1s 294us/step - loss: 0.0265 - acc: 0.9909\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 1s 288us/step - loss: 0.0183 - acc: 0.9943\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 1s 275us/step - loss: 0.0189 - acc: 0.9940\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 1s 251us/step - loss: 0.0141 - acc: 0.9958\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 1s 269us/step - loss: 0.0131 - acc: 0.9958\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 1s 230us/step - loss: 0.0145 - acc: 0.9951\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 1s 268us/step - loss: 0.0131 - acc: 0.9958\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0140 - acc: 0.9966\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0145 - acc: 0.9955\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.0138 - acc: 0.9962\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0133 - acc: 0.9958\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0176 - acc: 0.9940\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 1s 263us/step - loss: 0.0132 - acc: 0.9962\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 1s 257us/step - loss: 0.0166 - acc: 0.9943\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 1s 275us/step - loss: 0.0165 - acc: 0.9943\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 1s 248us/step - loss: 0.0163 - acc: 0.9951\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 1s 229us/step - loss: 0.0134 - acc: 0.9958\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 1s 278us/step - loss: 0.0132 - acc: 0.9966\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 1s 249us/step - loss: 0.0123 - acc: 0.9962\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 1s 271us/step - loss: 0.0167 - acc: 0.9932\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 1s 238us/step - loss: 0.0181 - acc: 0.9947\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 1s 246us/step - loss: 0.0234 - acc: 0.9928\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0168 - acc: 0.9936\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 1s 243us/step - loss: 0.0289 - acc: 0.9894\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0421 - acc: 0.9898\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 1s 248us/step - loss: 0.0240 - acc: 0.9932\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 1s 243us/step - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0123 - acc: 0.9966\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0121 - acc: 0.9962\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - 1s 261us/step - loss: 0.0120 - acc: 0.9962\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 1s 265us/step - loss: 0.0119 - acc: 0.9962\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0123 - acc: 0.9962\n",
      "295/295 [==============================] - 0s 282us/step\n",
      "Epoch 1/100\n",
      "2649/2649 [==============================] - 1s 433us/step - loss: 0.3368 - acc: 0.8732\n",
      "Epoch 2/100\n",
      "2649/2649 [==============================] - 1s 234us/step - loss: 0.2161 - acc: 0.9237\n",
      "Epoch 3/100\n",
      "2649/2649 [==============================] - 1s 247us/step - loss: 0.1933 - acc: 0.9287\n",
      "Epoch 4/100\n",
      "2649/2649 [==============================] - 1s 240us/step - loss: 0.1786 - acc: 0.9377\n",
      "Epoch 5/100\n",
      "2649/2649 [==============================] - 1s 301us/step - loss: 0.1732 - acc: 0.9336\n",
      "Epoch 6/100\n",
      "2649/2649 [==============================] - 1s 236us/step - loss: 0.1568 - acc: 0.9388\n",
      "Epoch 7/100\n",
      "2649/2649 [==============================] - 1s 264us/step - loss: 0.1542 - acc: 0.9400\n",
      "Epoch 8/100\n",
      "2649/2649 [==============================] - 1s 258us/step - loss: 0.1432 - acc: 0.9456\n",
      "Epoch 9/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.1479 - acc: 0.9404\n",
      "Epoch 10/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.1228 - acc: 0.9513\n",
      "Epoch 11/100\n",
      "2649/2649 [==============================] - 1s 249us/step - loss: 0.1214 - acc: 0.9498\n",
      "Epoch 12/100\n",
      "2649/2649 [==============================] - 1s 261us/step - loss: 0.1135 - acc: 0.9536\n",
      "Epoch 13/100\n",
      "2649/2649 [==============================] - 1s 314us/step - loss: 0.0994 - acc: 0.9622\n",
      "Epoch 14/100\n",
      "2649/2649 [==============================] - 1s 245us/step - loss: 0.1020 - acc: 0.9653\n",
      "Epoch 15/100\n",
      "2649/2649 [==============================] - 1s 193us/step - loss: 0.0875 - acc: 0.9656\n",
      "Epoch 16/100\n",
      "2649/2649 [==============================] - 1s 305us/step - loss: 0.0850 - acc: 0.9690\n",
      "Epoch 17/100\n",
      "2649/2649 [==============================] - 1s 286us/step - loss: 0.0808 - acc: 0.9702\n",
      "Epoch 18/100\n",
      "2649/2649 [==============================] - 1s 253us/step - loss: 0.0706 - acc: 0.9766\n",
      "Epoch 19/100\n",
      "2649/2649 [==============================] - 1s 206us/step - loss: 0.0650 - acc: 0.9773\n",
      "Epoch 20/100\n",
      "2649/2649 [==============================] - 1s 260us/step - loss: 0.0596 - acc: 0.9800\n",
      "Epoch 21/100\n",
      "2649/2649 [==============================] - 1s 264us/step - loss: 0.0666 - acc: 0.9736\n",
      "Epoch 22/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2649/2649 [==============================] - 1s 230us/step - loss: 0.0601 - acc: 0.9785\n",
      "Epoch 23/100\n",
      "2649/2649 [==============================] - 1s 255us/step - loss: 0.0524 - acc: 0.9823\n",
      "Epoch 24/100\n",
      "2649/2649 [==============================] - 1s 213us/step - loss: 0.0500 - acc: 0.9826\n",
      "Epoch 25/100\n",
      "2649/2649 [==============================] - 1s 289us/step - loss: 0.0499 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2649/2649 [==============================] - 1s 232us/step - loss: 0.0522 - acc: 0.9819\n",
      "Epoch 27/100\n",
      "2649/2649 [==============================] - 1s 201us/step - loss: 0.0547 - acc: 0.9777\n",
      "Epoch 28/100\n",
      "2649/2649 [==============================] - 1s 239us/step - loss: 0.0444 - acc: 0.9830\n",
      "Epoch 29/100\n",
      "2649/2649 [==============================] - 1s 252us/step - loss: 0.0408 - acc: 0.9834\n",
      "Epoch 30/100\n",
      "2649/2649 [==============================] - 1s 203us/step - loss: 0.0464 - acc: 0.9811\n",
      "Epoch 31/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0333 - acc: 0.9887\n",
      "Epoch 32/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.0322 - acc: 0.9887\n",
      "Epoch 33/100\n",
      "2649/2649 [==============================] - 1s 261us/step - loss: 0.0379 - acc: 0.9864\n",
      "Epoch 34/100\n",
      "2649/2649 [==============================] - 1s 261us/step - loss: 0.0381 - acc: 0.9864\n",
      "Epoch 35/100\n",
      "2649/2649 [==============================] - 1s 194us/step - loss: 0.0355 - acc: 0.9879\n",
      "Epoch 36/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0301 - acc: 0.9883\n",
      "Epoch 37/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0300 - acc: 0.9898\n",
      "Epoch 38/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0265 - acc: 0.9909\n",
      "Epoch 39/100\n",
      "2649/2649 [==============================] - 1s 244us/step - loss: 0.0274 - acc: 0.9913\n",
      "Epoch 40/100\n",
      "2649/2649 [==============================] - 1s 248us/step - loss: 0.0299 - acc: 0.9887\n",
      "Epoch 41/100\n",
      "2649/2649 [==============================] - 1s 271us/step - loss: 0.0400 - acc: 0.9849\n",
      "Epoch 42/100\n",
      "2649/2649 [==============================] - 1s 258us/step - loss: 0.0379 - acc: 0.9853\n",
      "Epoch 43/100\n",
      "2649/2649 [==============================] - 1s 250us/step - loss: 0.0306 - acc: 0.9894\n",
      "Epoch 44/100\n",
      "2649/2649 [==============================] - 1s 256us/step - loss: 0.0243 - acc: 0.9928\n",
      "Epoch 45/100\n",
      "2649/2649 [==============================] - 1s 231us/step - loss: 0.0238 - acc: 0.9906\n",
      "Epoch 46/100\n",
      "2649/2649 [==============================] - 1s 238us/step - loss: 0.0215 - acc: 0.9928\n",
      "Epoch 47/100\n",
      "2649/2649 [==============================] - 1s 250us/step - loss: 0.0232 - acc: 0.9921\n",
      "Epoch 48/100\n",
      "2649/2649 [==============================] - 1s 231us/step - loss: 0.0381 - acc: 0.9875\n",
      "Epoch 49/100\n",
      "2649/2649 [==============================] - 1s 215us/step - loss: 0.0221 - acc: 0.9917\n",
      "Epoch 50/100\n",
      "2649/2649 [==============================] - 1s 223us/step - loss: 0.0212 - acc: 0.9924\n",
      "Epoch 51/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.0215 - acc: 0.9924\n",
      "Epoch 52/100\n",
      "2649/2649 [==============================] - 1s 242us/step - loss: 0.0224 - acc: 0.9921\n",
      "Epoch 53/100\n",
      "2649/2649 [==============================] - 1s 232us/step - loss: 0.0187 - acc: 0.9940\n",
      "Epoch 54/100\n",
      "2649/2649 [==============================] - 1s 245us/step - loss: 0.0195 - acc: 0.9936\n",
      "Epoch 55/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.0174 - acc: 0.9947\n",
      "Epoch 56/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0178 - acc: 0.9943\n",
      "Epoch 57/100\n",
      "2649/2649 [==============================] - 1s 238us/step - loss: 0.0254 - acc: 0.9902\n",
      "Epoch 58/100\n",
      "2649/2649 [==============================] - 1s 264us/step - loss: 0.0224 - acc: 0.9924\n",
      "Epoch 59/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0233 - acc: 0.9902\n",
      "Epoch 60/100\n",
      "2649/2649 [==============================] - 1s 233us/step - loss: 0.0295 - acc: 0.9898\n",
      "Epoch 61/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0197 - acc: 0.9936\n",
      "Epoch 62/100\n",
      "2649/2649 [==============================] - 1s 205us/step - loss: 0.0169 - acc: 0.9936\n",
      "Epoch 63/100\n",
      "2649/2649 [==============================] - 0s 186us/step - loss: 0.0191 - acc: 0.9932\n",
      "Epoch 64/100\n",
      "2649/2649 [==============================] - 1s 192us/step - loss: 0.0174 - acc: 0.9947\n",
      "Epoch 65/100\n",
      "2649/2649 [==============================] - 1s 221us/step - loss: 0.0206 - acc: 0.9924\n",
      "Epoch 66/100\n",
      "2649/2649 [==============================] - 1s 197us/step - loss: 0.0180 - acc: 0.9932\n",
      "Epoch 67/100\n",
      "2649/2649 [==============================] - 0s 174us/step - loss: 0.0179 - acc: 0.9940\n",
      "Epoch 68/100\n",
      "2649/2649 [==============================] - 0s 185us/step - loss: 0.0162 - acc: 0.9943\n",
      "Epoch 69/100\n",
      "2649/2649 [==============================] - 1s 227us/step - loss: 0.0135 - acc: 0.9951\n",
      "Epoch 70/100\n",
      "2649/2649 [==============================] - 1s 197us/step - loss: 0.0161 - acc: 0.9928\n",
      "Epoch 71/100\n",
      "2649/2649 [==============================] - 1s 235us/step - loss: 0.0173 - acc: 0.9928\n",
      "Epoch 72/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0167 - acc: 0.9940\n",
      "Epoch 73/100\n",
      "2649/2649 [==============================] - 1s 258us/step - loss: 0.0161 - acc: 0.9943\n",
      "Epoch 74/100\n",
      "2649/2649 [==============================] - 1s 268us/step - loss: 0.0167 - acc: 0.9943\n",
      "Epoch 75/100\n",
      "2649/2649 [==============================] - 1s 228us/step - loss: 0.0145 - acc: 0.9947\n",
      "Epoch 76/100\n",
      "2649/2649 [==============================] - 1s 231us/step - loss: 0.0135 - acc: 0.9943\n",
      "Epoch 77/100\n",
      "2649/2649 [==============================] - 1s 256us/step - loss: 0.0437 - acc: 0.9834\n",
      "Epoch 78/100\n",
      "2649/2649 [==============================] - 1s 209us/step - loss: 0.0543 - acc: 0.9872\n",
      "Epoch 79/100\n",
      "2649/2649 [==============================] - 1s 216us/step - loss: 0.0237 - acc: 0.9917\n",
      "Epoch 80/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0180 - acc: 0.9936\n",
      "Epoch 81/100\n",
      "2649/2649 [==============================] - 1s 257us/step - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 82/100\n",
      "2649/2649 [==============================] - 1s 233us/step - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 83/100\n",
      "2649/2649 [==============================] - 1s 227us/step - loss: 0.0135 - acc: 0.9947\n",
      "Epoch 84/100\n",
      "2649/2649 [==============================] - 1s 212us/step - loss: 0.0168 - acc: 0.9943\n",
      "Epoch 85/100\n",
      "2649/2649 [==============================] - 1s 242us/step - loss: 0.0138 - acc: 0.9951\n",
      "Epoch 86/100\n",
      "2649/2649 [==============================] - 1s 246us/step - loss: 0.0159 - acc: 0.9936\n",
      "Epoch 87/100\n",
      "2649/2649 [==============================] - 1s 265us/step - loss: 0.0171 - acc: 0.9932\n",
      "Epoch 88/100\n",
      "2649/2649 [==============================] - 1s 225us/step - loss: 0.0185 - acc: 0.9928\n",
      "Epoch 89/100\n",
      "2649/2649 [==============================] - 1s 224us/step - loss: 0.0136 - acc: 0.9943\n",
      "Epoch 90/100\n",
      "2649/2649 [==============================] - 1s 207us/step - loss: 0.0134 - acc: 0.9955\n",
      "Epoch 91/100\n",
      "2649/2649 [==============================] - 1s 200us/step - loss: 0.0126 - acc: 0.9951\n",
      "Epoch 92/100\n",
      "2649/2649 [==============================] - 1s 237us/step - loss: 0.0146 - acc: 0.9940\n",
      "Epoch 93/100\n",
      "2649/2649 [==============================] - 1s 241us/step - loss: 0.0136 - acc: 0.9947\n",
      "Epoch 94/100\n",
      "2649/2649 [==============================] - 1s 222us/step - loss: 0.0130 - acc: 0.9947\n",
      "Epoch 95/100\n",
      "2649/2649 [==============================] - 1s 236us/step - loss: 0.0162 - acc: 0.9943\n",
      "Epoch 96/100\n",
      "2649/2649 [==============================] - 1s 237us/step - loss: 0.0135 - acc: 0.9958\n",
      "Epoch 97/100\n",
      "2649/2649 [==============================] - 1s 254us/step - loss: 0.0134 - acc: 0.9955\n",
      "Epoch 98/100\n",
      "2649/2649 [==============================] - ETA: 0s - loss: 0.0139 - acc: 0.995 - 1s 240us/step - loss: 0.0139 - acc: 0.9951\n",
      "Epoch 99/100\n",
      "2649/2649 [==============================] - 1s 220us/step - loss: 0.0231 - acc: 0.9913\n",
      "Epoch 100/100\n",
      "2649/2649 [==============================] - 1s 251us/step - loss: 0.0434 - acc: 0.9864\n",
      "295/295 [==============================] - 0s 389us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 2s 640us/step - loss: 0.3395 - acc: 0.8611\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.2003 - acc: 0.9260\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 1s 285us/step - loss: 0.1777 - acc: 0.9343\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.1749 - acc: 0.9321\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.1570 - acc: 0.9408\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.1446 - acc: 0.9434\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 273us/step - loss: 0.1441 - acc: 0.9438 0s - loss: 0.1507 - \n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 281us/step - loss: 0.1359 - acc: 0.9483\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.1246 - acc: 0.9547\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.1204 - acc: 0.9558\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.1101 - acc: 0.9581\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.1036 - acc: 0.9600\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 284us/step - loss: 0.0932 - acc: 0.9675\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0877 - acc: 0.9687\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0905 - acc: 0.9642\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0829 - acc: 0.9691\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 297us/step - loss: 0.0817 - acc: 0.9706\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 270us/step - loss: 0.0738 - acc: 0.9717\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.0665 - acc: 0.9743\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0634 - acc: 0.9762\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 280us/step - loss: 0.0578 - acc: 0.9808\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0516 - acc: 0.9826\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0474 - acc: 0.9826\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.0442 - acc: 0.9834\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 271us/step - loss: 0.0585 - acc: 0.9770\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.0485 - acc: 0.9842\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0487 - acc: 0.9857\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0421 - acc: 0.9838\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0403 - acc: 0.9857\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0430 - acc: 0.9842\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 264us/step - loss: 0.0344 - acc: 0.9891\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 297us/step - loss: 0.0283 - acc: 0.9891\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 280us/step - loss: 0.0343 - acc: 0.9879\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0483 - acc: 0.9823\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 189us/step - loss: 0.0316 - acc: 0.9875\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0287 - acc: 0.9902\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 191us/step - loss: 0.0327 - acc: 0.9887\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0273 - acc: 0.9917\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0242 - acc: 0.9909\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0240 - acc: 0.9917\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0335 - acc: 0.9894\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 1s 258us/step - loss: 0.0218 - acc: 0.9932\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0280 - acc: 0.9883\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0237 - acc: 0.9921\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0245 - acc: 0.9913\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 210us/step - loss: 0.0222 - acc: 0.9928\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0359 - acc: 0.9857\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0254 - acc: 0.9902\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0204 - acc: 0.9917\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 210us/step - loss: 0.0196 - acc: 0.9940\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0197 - acc: 0.9940\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0183 - acc: 0.9936\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0151 - acc: 0.9955\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0220 - acc: 0.9932\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0291 - acc: 0.9906\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 203us/step - loss: 0.0247 - acc: 0.9925\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0289 - acc: 0.9902\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 249us/step - loss: 0.0206 - acc: 0.9928\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0274 - acc: 0.9936\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0218 - acc: 0.9940\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0173 - acc: 0.9943\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0182 - acc: 0.9940\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.0170 - acc: 0.9943\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0175 - acc: 0.9943\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.0174 - acc: 0.9928\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 0s 187us/step - loss: 0.0182 - acc: 0.9936\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 194us/step - loss: 0.0156 - acc: 0.9940\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0158 - acc: 0.9943\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 1s 268us/step - loss: 0.0144 - acc: 0.9958\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0201 - acc: 0.9932\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0221 - acc: 0.9925\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0243 - acc: 0.9913\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0212 - acc: 0.9921\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0310 - acc: 0.9913\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0191 - acc: 0.9936\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0160 - acc: 0.9936\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 1s 223us/step - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 1s 189us/step - loss: 0.0139 - acc: 0.9958\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 1s 297us/step - loss: 0.0129 - acc: 0.9958\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 223us/step - loss: 0.0131 - acc: 0.9955\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0120 - acc: 0.9958\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.0205 - acc: 0.9940\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0130 - acc: 0.9951\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 1s 301us/step - loss: 0.0146 - acc: 0.9955\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 1s 275us/step - loss: 0.0138 - acc: 0.9947\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 273us/step - loss: 0.0138 - acc: 0.9955\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0124 - acc: 0.9958\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 197us/step - loss: 0.0127 - acc: 0.9951\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 1s 206us/step - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0141 - acc: 0.9951\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 1s 278us/step - loss: 0.0171 - acc: 0.9947\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 268us/step - loss: 0.0147 - acc: 0.9947\n",
      "294/294 [==============================] - 0s 456us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 1s 482us/step - loss: 0.3545 - acc: 0.8532\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 1s 216us/step - loss: 0.2052 - acc: 0.9215\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 1s 320us/step - loss: 0.1804 - acc: 0.9336\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 332us/step - loss: 0.1668 - acc: 0.9366\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.1588 - acc: 0.9400\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.1522 - acc: 0.9419\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.1447 - acc: 0.9442\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.1309 - acc: 0.9494\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.1280 - acc: 0.9509\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 286us/step - loss: 0.1237 - acc: 0.9555\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 306us/step - loss: 0.1189 - acc: 0.9536\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 194us/step - loss: 0.1068 - acc: 0.9581\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0970 - acc: 0.9615\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 249us/step - loss: 0.0873 - acc: 0.9675\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 268us/step - loss: 0.0874 - acc: 0.9675\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0720 - acc: 0.9755\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.0775 - acc: 0.9721\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0746 - acc: 0.9732\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 274us/step - loss: 0.0665 - acc: 0.9774\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0612 - acc: 0.9781\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0648 - acc: 0.9774\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0578 - acc: 0.9792\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0550 - acc: 0.9789\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 210us/step - loss: 0.0519 - acc: 0.9819\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.0540 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 229us/step - loss: 0.0490 - acc: 0.9819\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0574 - acc: 0.9804\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 205us/step - loss: 0.0487 - acc: 0.9838\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 209us/step - loss: 0.0415 - acc: 0.9838\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 198us/step - loss: 0.0439 - acc: 0.9849\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 201us/step - loss: 0.0442 - acc: 0.9849\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0366 - acc: 0.9868\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.0346 - acc: 0.9887\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 281us/step - loss: 0.0351 - acc: 0.9879\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 221us/step - loss: 0.0336 - acc: 0.9879\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0307 - acc: 0.9902\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 278us/step - loss: 0.0314 - acc: 0.9891\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 275us/step - loss: 0.0329 - acc: 0.9864\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0335 - acc: 0.9883\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 266us/step - loss: 0.0272 - acc: 0.9913\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0288 - acc: 0.9894\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 268us/step - loss: 0.0237 - acc: 0.9913\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0256 - acc: 0.9913\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 1s 224us/step - loss: 0.0261 - acc: 0.9891\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 1s 210us/step - loss: 0.0343 - acc: 0.9879\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 1s 211us/step - loss: 0.0252 - acc: 0.9909\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.0270 - acc: 0.9891\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0330 - acc: 0.9898\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0444 - acc: 0.9834\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0300 - acc: 0.9902\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 193us/step - loss: 0.0235 - acc: 0.9921\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 229us/step - loss: 0.0286 - acc: 0.9902\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 0s 182us/step - loss: 0.0262 - acc: 0.9898\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0199 - acc: 0.9932\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.0200 - acc: 0.9943\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0180 - acc: 0.9940\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0213 - acc: 0.9925\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0181 - acc: 0.9943\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0178 - acc: 0.9940\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0167 - acc: 0.9951\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 211us/step - loss: 0.0168 - acc: 0.9951\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0227 - acc: 0.9906\n",
      "Epoch 63/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0232 - acc: 0.9913\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.0209 - acc: 0.9917\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0231 - acc: 0.9925\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 314us/step - loss: 0.0187 - acc: 0.9940\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 274us/step - loss: 0.0167 - acc: 0.9940\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0289 - acc: 0.9894\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0224 - acc: 0.9909\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 1s 293us/step - loss: 0.0176 - acc: 0.9951\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0229 - acc: 0.9921 0s - loss: 0.0312 - ac\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0169 - acc: 0.9936\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.0152 - acc: 0.9947\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 0s 188us/step - loss: 0.0182 - acc: 0.9940\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0144 - acc: 0.9955\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 0s 178us/step - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 194us/step - loss: 0.0171 - acc: 0.9940\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 0s 167us/step - loss: 0.0159 - acc: 0.9947\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 285us/step - loss: 0.0164 - acc: 0.9943\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0148 - acc: 0.9955\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 1s 275us/step - loss: 0.0165 - acc: 0.9932\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0159 - acc: 0.9947\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.0188 - acc: 0.9932\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0609 - acc: 0.9830\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 297us/step - loss: 0.0344 - acc: 0.9887\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0202 - acc: 0.9932\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0149 - acc: 0.9955\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0147 - acc: 0.9955\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0151 - acc: 0.9951\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0136 - acc: 0.9955\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 1s 263us/step - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0153 - acc: 0.9947\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 1s 314us/step - loss: 0.0140 - acc: 0.9958\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 1s 369us/step - loss: 0.0194 - acc: 0.9928\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 1s 374us/step - loss: 0.0213 - acc: 0.9936\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 565us/step - loss: 0.0233 - acc: 0.9917\n",
      "294/294 [==============================] - 0s 525us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 2s 815us/step - loss: 0.3501 - acc: 0.8785\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.2046 - acc: 0.9260\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.1850 - acc: 0.9325\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.1865 - acc: 0.9249\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.1639 - acc: 0.9347\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.1578 - acc: 0.9419\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 311us/step - loss: 0.1414 - acc: 0.9468\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 209us/step - loss: 0.1339 - acc: 0.9502\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.1256 - acc: 0.9566\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.1200 - acc: 0.9547\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.1143 - acc: 0.9562\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 331us/step - loss: 0.1132 - acc: 0.9540\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.1030 - acc: 0.9596\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.1012 - acc: 0.9604\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 211us/step - loss: 0.0868 - acc: 0.9664\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0790 - acc: 0.9713\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0733 - acc: 0.9732\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.0709 - acc: 0.9717\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 271us/step - loss: 0.0644 - acc: 0.9774\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 280us/step - loss: 0.0648 - acc: 0.9728\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0677 - acc: 0.9728\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0626 - acc: 0.9751\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 1s 257us/step - loss: 0.0604 - acc: 0.9770\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 273us/step - loss: 0.0528 - acc: 0.9819\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 247us/step - loss: 0.0508 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0598 - acc: 0.9781\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0547 - acc: 0.9785\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0460 - acc: 0.9830\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 281us/step - loss: 0.0447 - acc: 0.9838\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 282us/step - loss: 0.0388 - acc: 0.9872\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 224us/step - loss: 0.0401 - acc: 0.9864\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0331 - acc: 0.9883\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 198us/step - loss: 0.0422 - acc: 0.9849\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0383 - acc: 0.9868\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0341 - acc: 0.9887\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 216us/step - loss: 0.0295 - acc: 0.9906\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 205us/step - loss: 0.0289 - acc: 0.9909\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 204us/step - loss: 0.0311 - acc: 0.9902\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.0249 - acc: 0.993 - 1s 196us/step - loss: 0.0245 - acc: 0.9932\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 205us/step - loss: 0.0352 - acc: 0.9894\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0296 - acc: 0.9917\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0284 - acc: 0.9902\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 0s 187us/step - loss: 0.0254 - acc: 0.9921\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 0s 181us/step - loss: 0.0276 - acc: 0.9913\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 0s 174us/step - loss: 0.0222 - acc: 0.9932\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 0s 187us/step - loss: 0.0291 - acc: 0.9902\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0314 - acc: 0.9925\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 201us/step - loss: 0.0242 - acc: 0.9909\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 247us/step - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.0244 - acc: 0.9917\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 264us/step - loss: 0.0198 - acc: 0.9940\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0215 - acc: 0.9928\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 1s 249us/step - loss: 0.0184 - acc: 0.9943\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 193us/step - loss: 0.0198 - acc: 0.9943\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 0s 186us/step - loss: 0.0191 - acc: 0.9940\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 221us/step - loss: 0.0291 - acc: 0.9909\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0379 - acc: 0.9875\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.0217 - acc: 0.9936\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 206us/step - loss: 0.0237 - acc: 0.9909\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 218us/step - loss: 0.0175 - acc: 0.9947\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0202 - acc: 0.9925\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0188 - acc: 0.9943\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.0339 - acc: 0.9917\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0205 - acc: 0.9936\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0232 - acc: 0.9921\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0189 - acc: 0.9928\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 218us/step - loss: 0.0150 - acc: 0.9951\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0153 - acc: 0.9955\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 1s 280us/step - loss: 0.0182 - acc: 0.9951\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 1s 306us/step - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0155 - acc: 0.9951\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0162 - acc: 0.9947\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 200us/step - loss: 0.0158 - acc: 0.9955\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 207us/step - loss: 0.0146 - acc: 0.9962\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 1s 211us/step - loss: 0.0154 - acc: 0.9947\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0131 - acc: 0.9962\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 1s 278us/step - loss: 0.0150 - acc: 0.9947\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 324us/step - loss: 0.0145 - acc: 0.9955\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 1s 288us/step - loss: 0.0175 - acc: 0.9940\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 258us/step - loss: 0.0190 - acc: 0.9940\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 257us/step - loss: 0.0184 - acc: 0.9936\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 1s 209us/step - loss: 0.0259 - acc: 0.9921\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 0s 186us/step - loss: 0.0282 - acc: 0.9906\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 0s 180us/step - loss: 0.0258 - acc: 0.9932\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 0s 179us/step - loss: 0.0359 - acc: 0.9909\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0189 - acc: 0.9955\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 249us/step - loss: 0.0144 - acc: 0.9955\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0158 - acc: 0.9947\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0133 - acc: 0.9966\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.0135 - acc: 0.9962\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0140 - acc: 0.9962\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0132 - acc: 0.9955\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 286us/step - loss: 0.0129 - acc: 0.9962\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0145 - acc: 0.9947\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 1s 301us/step - loss: 0.0148 - acc: 0.9958\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.0127 - acc: 0.9962\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0124 - acc: 0.9962\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0121 - acc: 0.9962\n",
      "294/294 [==============================] - 0s 511us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 2s 572us/step - loss: 0.3429 - acc: 0.8845\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 0s 185us/step - loss: 0.2024 - acc: 0.9253\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 1s 203us/step - loss: 0.1847 - acc: 0.9309\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.1716 - acc: 0.9370\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.1637 - acc: 0.9366\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.1478 - acc: 0.9423\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.1447 - acc: 0.9430\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 375us/step - loss: 0.1306 - acc: 0.9547 0s - loss: 0.1321\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 277us/step - loss: 0.1244 - acc: 0.9521\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 286us/step - loss: 0.1136 - acc: 0.9570 0s - loss: 0.0973 -\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 326us/step - loss: 0.1027 - acc: 0.9611\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 269us/step - loss: 0.1005 - acc: 0.9608\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.0898 - acc: 0.9657\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 284us/step - loss: 0.0796 - acc: 0.9709\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 229us/step - loss: 0.0785 - acc: 0.9725\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.0731 - acc: 0.9721\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0719 - acc: 0.9766\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0658 - acc: 0.9743\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 267us/step - loss: 0.0575 - acc: 0.9808\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0532 - acc: 0.9811\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 207us/step - loss: 0.0855 - acc: 0.9694\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.0599 - acc: 0.9800\n",
      "Epoch 23/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0477 - acc: 0.9849\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.0488 - acc: 0.9823\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 297us/step - loss: 0.0479 - acc: 0.9845\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.0511 - acc: 0.9826\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0409 - acc: 0.9860\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0448 - acc: 0.9849\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0440 - acc: 0.9853\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0342 - acc: 0.9887\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 195us/step - loss: 0.0366 - acc: 0.9868\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.0293 - acc: 0.9906\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 203us/step - loss: 0.0297 - acc: 0.9894\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 198us/step - loss: 0.0288 - acc: 0.9898\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0293 - acc: 0.9894\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0269 - acc: 0.9906\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 320us/step - loss: 0.0342 - acc: 0.9875\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 539us/step - loss: 0.0274 - acc: 0.9902\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 1s 291us/step - loss: 0.0280 - acc: 0.9913\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.0332 - acc: 0.9883\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0376 - acc: 0.9875\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0238 - acc: 0.9917\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.0220 - acc: 0.9917\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 1s 301us/step - loss: 0.0281 - acc: 0.9902\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 1s 279us/step - loss: 0.0258 - acc: 0.9913\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0233 - acc: 0.9925\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0214 - acc: 0.9932\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0211 - acc: 0.9932\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0232 - acc: 0.9913\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0242 - acc: 0.9913\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 201us/step - loss: 0.0213 - acc: 0.9928\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0273 - acc: 0.9909\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 1s 201us/step - loss: 0.0417 - acc: 0.9849\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.0238 - acc: 0.9902\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 1s 247us/step - loss: 0.0256 - acc: 0.9909\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0215 - acc: 0.9940\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.0184 - acc: 0.9940\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0179 - acc: 0.9936\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 229us/step - loss: 0.0190 - acc: 0.9936\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 205us/step - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0162 - acc: 0.9940\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.0195 - acc: 0.9932\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0177 - acc: 0.9932\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0207 - acc: 0.9921\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.0158 - acc: 0.9947\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.0156 - acc: 0.9940\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0158 - acc: 0.9943\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0177 - acc: 0.9940\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0477 - acc: 0.9853\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.0260 - acc: 0.9928\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 221us/step - loss: 0.0276 - acc: 0.9891\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0207 - acc: 0.9947\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.0181 - acc: 0.9936\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 200us/step - loss: 0.0143 - acc: 0.9951\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 1s 221us/step - loss: 0.0169 - acc: 0.9928\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0227 - acc: 0.9928\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 1s 271us/step - loss: 0.0161 - acc: 0.9947\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0153 - acc: 0.9955\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0146 - acc: 0.9951\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0152 - acc: 0.9955\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0187 - acc: 0.9928\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0199 - acc: 0.9928\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0156 - acc: 0.9943\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.0138 - acc: 0.9943\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0145 - acc: 0.9951\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0163 - acc: 0.9940\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0147 - acc: 0.9951\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 253us/step - loss: 0.0191 - acc: 0.9921\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 209us/step - loss: 0.0204 - acc: 0.9928\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0156 - acc: 0.9951\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 0s 172us/step - loss: 0.0451 - acc: 0.9875\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 0s 179us/step - loss: 0.0214 - acc: 0.9913\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0177 - acc: 0.9936\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 257us/step - loss: 0.0143 - acc: 0.9955\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0142 - acc: 0.9947\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.0126 - acc: 0.9958\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 1s 191us/step - loss: 0.0117 - acc: 0.9966\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.0125 - acc: 0.9955\n",
      "294/294 [==============================] - 0s 621us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 2s 570us/step - loss: 0.3455 - acc: 0.8570\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 1s 316us/step - loss: 0.2103 - acc: 0.9230\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 253us/step - loss: 0.1851 - acc: 0.9347\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.1720 - acc: 0.9400\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.1658 - acc: 0.9426\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.1614 - acc: 0.9374\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.1499 - acc: 0.9426\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.1398 - acc: 0.9483\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.1310 - acc: 0.9521\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.1272 - acc: 0.9521\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 257us/step - loss: 0.1233 - acc: 0.9577\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 206us/step - loss: 0.1109 - acc: 0.9585\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.1054 - acc: 0.9638\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.0967 - acc: 0.9657\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0955 - acc: 0.9683\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0891 - acc: 0.9657\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.0782 - acc: 0.9713\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 266us/step - loss: 0.0712 - acc: 0.9732\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0674 - acc: 0.9766\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0702 - acc: 0.9728\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0628 - acc: 0.9792\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0602 - acc: 0.9789\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0536 - acc: 0.9811\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0529 - acc: 0.9811\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0537 - acc: 0.9815\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0495 - acc: 0.9823\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0469 - acc: 0.9842\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0403 - acc: 0.9857\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 249us/step - loss: 0.0374 - acc: 0.9879\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0406 - acc: 0.9849\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0402 - acc: 0.9868\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 228us/step - loss: 0.0460 - acc: 0.9830\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0345 - acc: 0.9887\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0297 - acc: 0.9917\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0304 - acc: 0.9917\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0294 - acc: 0.9887\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 262us/step - loss: 0.0298 - acc: 0.9883\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0364 - acc: 0.9898\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 1s 258us/step - loss: 0.0283 - acc: 0.9883\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0272 - acc: 0.9917\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 267us/step - loss: 0.0291 - acc: 0.9898\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0283 - acc: 0.9921\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0452 - acc: 0.9868\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 1s 261us/step - loss: 0.0305 - acc: 0.9891\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0227 - acc: 0.9925\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 1s 257us/step - loss: 0.0226 - acc: 0.9928\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 223us/step - loss: 0.0237 - acc: 0.9921\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0231 - acc: 0.9928\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 277us/step - loss: 0.0207 - acc: 0.9940\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0212 - acc: 0.9928\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 359us/step - loss: 0.0207 - acc: 0.9940\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 328us/step - loss: 0.0214 - acc: 0.9928\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 253us/step - loss: 0.0292 - acc: 0.9868\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 1s 278us/step - loss: 0.0284 - acc: 0.9909\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 276us/step - loss: 0.0188 - acc: 0.9940\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.0186 - acc: 0.9932\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0224 - acc: 0.9928\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0190 - acc: 0.9925\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 251us/step - loss: 0.0186 - acc: 0.9940\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 275us/step - loss: 0.0191 - acc: 0.9925\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0190 - acc: 0.9943\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0253 - acc: 0.9917\n",
      "Epoch 64/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0385 - acc: 0.9883\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0345 - acc: 0.9883\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 294us/step - loss: 0.0208 - acc: 0.9932\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 248us/step - loss: 0.0185 - acc: 0.9936\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 247us/step - loss: 0.0217 - acc: 0.9921\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 212us/step - loss: 0.0171 - acc: 0.9943\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - 1s 223us/step - loss: 0.0157 - acc: 0.9947\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 1s 288us/step - loss: 0.0147 - acc: 0.9951\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 266us/step - loss: 0.0142 - acc: 0.9951\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 310us/step - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 242us/step - loss: 0.0161 - acc: 0.9955\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 238us/step - loss: 0.0152 - acc: 0.9947\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0162 - acc: 0.9947\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0353 - acc: 0.9887\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 1s 263us/step - loss: 0.0194 - acc: 0.9928\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0164 - acc: 0.9947\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 1s 288us/step - loss: 0.0153 - acc: 0.9947\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 264us/step - loss: 0.0212 - acc: 0.9928\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0156 - acc: 0.9943\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0142 - acc: 0.9951\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 267us/step - loss: 0.0148 - acc: 0.9955 0s - loss: 0.0131 - \n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 1s 274us/step - loss: 0.0138 - acc: 0.9947\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0134 - acc: 0.9947\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.0148 - acc: 0.9947\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0136 - acc: 0.9947\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 237us/step - loss: 0.0153 - acc: 0.9943\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 278us/step - loss: 0.0275 - acc: 0.9891\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0313 - acc: 0.9909\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.0192 - acc: 0.992 - 1s 243us/step - loss: 0.0182 - acc: 0.9928\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0181 - acc: 0.9940\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 227us/step - loss: 0.0127 - acc: 0.9958\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 222us/step - loss: 0.0143 - acc: 0.9943\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 265us/step - loss: 0.0141 - acc: 0.9940\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 0s 179us/step - loss: 0.0135 - acc: 0.9955\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 0s 168us/step - loss: 0.0141 - acc: 0.9955\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 0s 182us/step - loss: 0.0137 - acc: 0.9955\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0159 - acc: 0.9936\n",
      "294/294 [==============================] - 0s 635us/step\n",
      "Epoch 1/100\n",
      "2650/2650 [==============================] - 2s 568us/step - loss: 0.3477 - acc: 0.8479\n",
      "Epoch 2/100\n",
      "2650/2650 [==============================] - 1s 263us/step - loss: 0.2067 - acc: 0.9283\n",
      "Epoch 3/100\n",
      "2650/2650 [==============================] - 1s 260us/step - loss: 0.1906 - acc: 0.9309\n",
      "Epoch 4/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.1813 - acc: 0.9362\n",
      "Epoch 5/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.1739 - acc: 0.9389\n",
      "Epoch 6/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.1595 - acc: 0.9377\n",
      "Epoch 7/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.1493 - acc: 0.9445\n",
      "Epoch 8/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.1469 - acc: 0.9430\n",
      "Epoch 9/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.1349 - acc: 0.9479\n",
      "Epoch 10/100\n",
      "2650/2650 [==============================] - 1s 247us/step - loss: 0.1192 - acc: 0.9558\n",
      "Epoch 11/100\n",
      "2650/2650 [==============================] - 1s 250us/step - loss: 0.1166 - acc: 0.9558\n",
      "Epoch 12/100\n",
      "2650/2650 [==============================] - 1s 196us/step - loss: 0.1068 - acc: 0.9585\n",
      "Epoch 13/100\n",
      "2650/2650 [==============================] - 1s 195us/step - loss: 0.0961 - acc: 0.9642\n",
      "Epoch 14/100\n",
      "2650/2650 [==============================] - 1s 190us/step - loss: 0.0988 - acc: 0.9638\n",
      "Epoch 15/100\n",
      "2650/2650 [==============================] - 1s 218us/step - loss: 0.0894 - acc: 0.9687\n",
      "Epoch 16/100\n",
      "2650/2650 [==============================] - 1s 295us/step - loss: 0.0810 - acc: 0.9713\n",
      "Epoch 17/100\n",
      "2650/2650 [==============================] - 1s 252us/step - loss: 0.0761 - acc: 0.9717\n",
      "Epoch 18/100\n",
      "2650/2650 [==============================] - 1s 206us/step - loss: 0.0720 - acc: 0.9740\n",
      "Epoch 19/100\n",
      "2650/2650 [==============================] - 1s 192us/step - loss: 0.0640 - acc: 0.9758\n",
      "Epoch 20/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0609 - acc: 0.9781\n",
      "Epoch 21/100\n",
      "2650/2650 [==============================] - 1s 284us/step - loss: 0.0655 - acc: 0.9755\n",
      "Epoch 22/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0564 - acc: 0.9789\n",
      "Epoch 23/100\n",
      "2650/2650 [==============================] - 1s 286us/step - loss: 0.0541 - acc: 0.9819\n",
      "Epoch 24/100\n",
      "2650/2650 [==============================] - 1s 279us/step - loss: 0.0475 - acc: 0.9853\n",
      "Epoch 25/100\n",
      "2650/2650 [==============================] - 1s 431us/step - loss: 0.0594 - acc: 0.9808\n",
      "Epoch 26/100\n",
      "2650/2650 [==============================] - 1s 368us/step - loss: 0.0481 - acc: 0.9830\n",
      "Epoch 27/100\n",
      "2650/2650 [==============================] - 1s 338us/step - loss: 0.0412 - acc: 0.9879\n",
      "Epoch 28/100\n",
      "2650/2650 [==============================] - 1s 244us/step - loss: 0.0482 - acc: 0.9830\n",
      "Epoch 29/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0504 - acc: 0.9800\n",
      "Epoch 30/100\n",
      "2650/2650 [==============================] - 1s 255us/step - loss: 0.0376 - acc: 0.9868\n",
      "Epoch 31/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0399 - acc: 0.9860\n",
      "Epoch 32/100\n",
      "2650/2650 [==============================] - 1s 231us/step - loss: 0.0357 - acc: 0.9875\n",
      "Epoch 33/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0308 - acc: 0.9902\n",
      "Epoch 34/100\n",
      "2650/2650 [==============================] - 1s 264us/step - loss: 0.0379 - acc: 0.9857\n",
      "Epoch 35/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0307 - acc: 0.9898\n",
      "Epoch 36/100\n",
      "2650/2650 [==============================] - 1s 259us/step - loss: 0.0293 - acc: 0.9872\n",
      "Epoch 37/100\n",
      "2650/2650 [==============================] - 1s 272us/step - loss: 0.0285 - acc: 0.9902\n",
      "Epoch 38/100\n",
      "2650/2650 [==============================] - 1s 266us/step - loss: 0.0306 - acc: 0.9894\n",
      "Epoch 39/100\n",
      "2650/2650 [==============================] - 1s 283us/step - loss: 0.0251 - acc: 0.9921\n",
      "Epoch 40/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0257 - acc: 0.9921\n",
      "Epoch 41/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0247 - acc: 0.9928\n",
      "Epoch 42/100\n",
      "2650/2650 [==============================] - 1s 214us/step - loss: 0.0218 - acc: 0.9928\n",
      "Epoch 43/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0260 - acc: 0.9902\n",
      "Epoch 44/100\n",
      "2650/2650 [==============================] - 1s 241us/step - loss: 0.0361 - acc: 0.9909\n",
      "Epoch 45/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.0335 - acc: 0.9883\n",
      "Epoch 46/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0294 - acc: 0.9898\n",
      "Epoch 47/100\n",
      "2650/2650 [==============================] - 1s 207us/step - loss: 0.0220 - acc: 0.9913\n",
      "Epoch 48/100\n",
      "2650/2650 [==============================] - 1s 203us/step - loss: 0.0202 - acc: 0.9932\n",
      "Epoch 49/100\n",
      "2650/2650 [==============================] - 1s 370us/step - loss: 0.0210 - acc: 0.9928\n",
      "Epoch 50/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0259 - acc: 0.9906\n",
      "Epoch 51/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0314 - acc: 0.9883\n",
      "Epoch 52/100\n",
      "2650/2650 [==============================] - 1s 230us/step - loss: 0.0241 - acc: 0.9917\n",
      "Epoch 53/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0258 - acc: 0.9909\n",
      "Epoch 54/100\n",
      "2650/2650 [==============================] - 1s 203us/step - loss: 0.0193 - acc: 0.9940\n",
      "Epoch 55/100\n",
      "2650/2650 [==============================] - 1s 334us/step - loss: 0.0266 - acc: 0.9913\n",
      "Epoch 56/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0218 - acc: 0.9921\n",
      "Epoch 57/100\n",
      "2650/2650 [==============================] - 1s 209us/step - loss: 0.0266 - acc: 0.9909\n",
      "Epoch 58/100\n",
      "2650/2650 [==============================] - 1s 279us/step - loss: 0.0255 - acc: 0.9917\n",
      "Epoch 59/100\n",
      "2650/2650 [==============================] - 1s 312us/step - loss: 0.0244 - acc: 0.9921\n",
      "Epoch 60/100\n",
      "2650/2650 [==============================] - 1s 309us/step - loss: 0.0183 - acc: 0.9940\n",
      "Epoch 61/100\n",
      "2650/2650 [==============================] - 1s 225us/step - loss: 0.0179 - acc: 0.9943\n",
      "Epoch 62/100\n",
      "2650/2650 [==============================] - 1s 220us/step - loss: 0.0170 - acc: 0.9936\n",
      "Epoch 63/100\n",
      "2650/2650 [==============================] - 1s 221us/step - loss: 0.0214 - acc: 0.9925\n",
      "Epoch 64/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2650/2650 [==============================] - 1s 213us/step - loss: 0.0160 - acc: 0.9947\n",
      "Epoch 65/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0174 - acc: 0.9943\n",
      "Epoch 66/100\n",
      "2650/2650 [==============================] - 1s 235us/step - loss: 0.0162 - acc: 0.9943\n",
      "Epoch 67/100\n",
      "2650/2650 [==============================] - 1s 337us/step - loss: 0.0178 - acc: 0.9936\n",
      "Epoch 68/100\n",
      "2650/2650 [==============================] - 1s 239us/step - loss: 0.0157 - acc: 0.9943\n",
      "Epoch 69/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0174 - acc: 0.9940\n",
      "Epoch 70/100\n",
      "2650/2650 [==============================] - ETA: 0s - loss: 0.0211 - acc: 0.992 - 1s 218us/step - loss: 0.0212 - acc: 0.9921\n",
      "Epoch 71/100\n",
      "2650/2650 [==============================] - 1s 218us/step - loss: 0.0334 - acc: 0.9906\n",
      "Epoch 72/100\n",
      "2650/2650 [==============================] - 1s 258us/step - loss: 0.0189 - acc: 0.9936\n",
      "Epoch 73/100\n",
      "2650/2650 [==============================] - 1s 315us/step - loss: 0.0203 - acc: 0.9921\n",
      "Epoch 74/100\n",
      "2650/2650 [==============================] - 1s 245us/step - loss: 0.0225 - acc: 0.9906\n",
      "Epoch 75/100\n",
      "2650/2650 [==============================] - 1s 243us/step - loss: 0.0177 - acc: 0.9936\n",
      "Epoch 76/100\n",
      "2650/2650 [==============================] - 1s 211us/step - loss: 0.0161 - acc: 0.9951\n",
      "Epoch 77/100\n",
      "2650/2650 [==============================] - 1s 246us/step - loss: 0.0149 - acc: 0.9951\n",
      "Epoch 78/100\n",
      "2650/2650 [==============================] - 1s 256us/step - loss: 0.0149 - acc: 0.9943\n",
      "Epoch 79/100\n",
      "2650/2650 [==============================] - 1s 215us/step - loss: 0.0169 - acc: 0.9928\n",
      "Epoch 80/100\n",
      "2650/2650 [==============================] - 1s 253us/step - loss: 0.0454 - acc: 0.9875\n",
      "Epoch 81/100\n",
      "2650/2650 [==============================] - 1s 219us/step - loss: 0.0413 - acc: 0.9883\n",
      "Epoch 82/100\n",
      "2650/2650 [==============================] - 1s 204us/step - loss: 0.0176 - acc: 0.9955\n",
      "Epoch 83/100\n",
      "2650/2650 [==============================] - 1s 233us/step - loss: 0.0153 - acc: 0.9951\n",
      "Epoch 84/100\n",
      "2650/2650 [==============================] - 1s 283us/step - loss: 0.0150 - acc: 0.9955\n",
      "Epoch 85/100\n",
      "2650/2650 [==============================] - 1s 236us/step - loss: 0.0145 - acc: 0.9951\n",
      "Epoch 86/100\n",
      "2650/2650 [==============================] - 1s 234us/step - loss: 0.0142 - acc: 0.9947\n",
      "Epoch 87/100\n",
      "2650/2650 [==============================] - 1s 232us/step - loss: 0.0142 - acc: 0.9943\n",
      "Epoch 88/100\n",
      "2650/2650 [==============================] - 1s 218us/step - loss: 0.0145 - acc: 0.9951\n",
      "Epoch 89/100\n",
      "2650/2650 [==============================] - 1s 195us/step - loss: 0.0133 - acc: 0.9955\n",
      "Epoch 90/100\n",
      "2650/2650 [==============================] - 1s 208us/step - loss: 0.0177 - acc: 0.9936\n",
      "Epoch 91/100\n",
      "2650/2650 [==============================] - 1s 254us/step - loss: 0.0149 - acc: 0.9943\n",
      "Epoch 92/100\n",
      "2650/2650 [==============================] - 1s 289us/step - loss: 0.0165 - acc: 0.9947\n",
      "Epoch 93/100\n",
      "2650/2650 [==============================] - 1s 240us/step - loss: 0.0161 - acc: 0.9943 0s - loss: 0.0072 - a\n",
      "Epoch 94/100\n",
      "2650/2650 [==============================] - 1s 217us/step - loss: 0.0163 - acc: 0.9940\n",
      "Epoch 95/100\n",
      "2650/2650 [==============================] - 1s 204us/step - loss: 0.0139 - acc: 0.9951\n",
      "Epoch 96/100\n",
      "2650/2650 [==============================] - 1s 224us/step - loss: 0.0143 - acc: 0.9940\n",
      "Epoch 97/100\n",
      "2650/2650 [==============================] - 1s 263us/step - loss: 0.0166 - acc: 0.9940\n",
      "Epoch 98/100\n",
      "2650/2650 [==============================] - 1s 194us/step - loss: 0.0135 - acc: 0.9958 0s - loss: 0.0170 - acc:\n",
      "Epoch 99/100\n",
      "2650/2650 [==============================] - 1s 202us/step - loss: 0.0153 - acc: 0.9943\n",
      "Epoch 100/100\n",
      "2650/2650 [==============================] - 1s 226us/step - loss: 0.0138 - acc: 0.9958\n",
      "294/294 [==============================] - 0s 604us/step\n",
      "Accuracy mean: 0.940888965735\n",
      "Accuracy variance: 0.0100348108888\n",
      "(' Time ', '644.217', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()  \n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential # initialize neural network library\n",
    "from keras.layers import Dense # build our layers library\n",
    "def build_classifier():\n",
    "    classifier = Sequential() # initialize neural network\n",
    "    classifier.add(Dense(units = 1024, kernel_initializer = 'uniform', activation = 'relu', input_dim = X_train.shape[1]))\n",
    "    classifier.add(Dense(units = 512, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    \n",
    "    classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return classifier\n",
    "classifier = KerasClassifier(build_fn = build_classifier, epochs = 100)\n",
    "accuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "print(\"Accuracy mean: \"+ str(mean))\n",
    "print(\"Accuracy variance: \"+ str(variance)) \n",
    "tt = time() - t0  \n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of DecisionTreeClassifier = 90.102930 %\n",
      "(' Time ', '0.084', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of DecisionTreeClassifier = {:.6f} %\".format(acc * 100))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of RandomForestClassifier = 94.615994 %\n",
      "(' Time ', '0.725', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of RandomForestClassifier = {:.6f} %\".format(acc * 100))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExtraTreesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of ExtraTreesClassifier = 94.615994 %\n",
      "(' Time ', '0.653', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "clf = ExtraTreesClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of ExtraTreesClassifier = {:.6f} %\".format(acc * 100))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of AdaBoostClassifier = 94.378464 %\n",
      "(' Time ', '0.695', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of AdaBoostClassifier = {:.6f} %\".format(acc * 100))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Naive_bayes  GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of GaussianNB = 81.631037 %\n",
      "(' Time ', '0.006', ' seconds')\n"
     ]
    }
   ],
   "source": [
    "t0 = time()\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb = gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred= gnb.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"Test Accuracy of GaussianNB = {:.6f} %\".format(acc * 100))\n",
    "tt = time() - t0\n",
    "print (\" Time \",format(round(tt,3)),\" seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
